{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook\n"
     ]
    }
   ],
   "source": [
    "import plotly.io as pio\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"Running as a Colab notebook\")\n",
    "    pio.renderers.default = \"colab\"\n",
    "    %pip install transformer-lens fancy-einsum\n",
    "    %pip install -U kaleido # kaleido only works if you restart the runtime. Required to write figures to disk (final cell)\n",
    "except:\n",
    "    print(\"Running as a Jupyter notebook\")\n",
    "    pio.renderers.default = \"vscode\"\n",
    "    from IPython import get_ipython\n",
    "    ipython = get_ipython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from fancy_einsum import einsum\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils, ActivationCache\n",
    "from torchtyping import TensorType as TT\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import einops\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # https://stackoverflow.com/q/62691279\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: circuitsvis in /Users/clementneo/projects/aan/aan/lib/python3.10/site-packages (1.39.1)\n",
      "Requirement already satisfied: torch<2.0,>=1.10 in /Users/clementneo/projects/aan/aan/lib/python3.10/site-packages (from circuitsvis) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /Users/clementneo/projects/aan/aan/lib/python3.10/site-packages (from circuitsvis) (5.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.23 in /Users/clementneo/projects/aan/aan/lib/python3.10/site-packages (from circuitsvis) (1.24.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/clementneo/projects/aan/aan/lib/python3.10/site-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/clementneo/projects/aan/aan/lib/python3.10/site-packages (from torch<2.0,>=1.10->circuitsvis) (4.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install circuitsvis\n",
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.renderers.default='vscode'\n",
    "\n",
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-large into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-large\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Prompts For GPT-4 Example Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLANATION_JSON_FILE_PATH = \"data/head_explanation_1_nh_to_exp.json\"\n",
    "\n",
    "# Load results\n",
    "with open(EXPLANATION_JSON_FILE_PATH) as f:\n",
    "    nh_to_exp_str_key = json.load(f)\n",
    "\n",
    "# The key of the response dict is a string '((a, b), c)' where a, b, c are integers. COnvert to tuple of (a, b, c).\n",
    "nh_to_exp = {tuple(int(value.strip()) for value in k.strip('()').split(',')):v for k,v in nh_to_exp_str_key.items()}\n",
    "\n",
    "# Load neuron word (inside neuron_finder_results)\n",
    "with open(\"data/neuron_finder_results.json\", \"r\") as f:\n",
    "    neuron_finder_results = json.load(f)\n",
    "\n",
    "neuron_to_token = {}\n",
    "\n",
    "for layer in neuron_finder_results.keys():\n",
    "    for neuron_ind in neuron_finder_results[layer].keys():\n",
    "        neuron_to_token[(int(layer), int(neuron_ind))] = neuron_finder_results[layer][neuron_ind][0]\n",
    "\n",
    "# We have nh_to_exp, which is a dict of the form (layer, neuron, head) -> explanation string\n",
    "# and neuron_to_token, which is a dict of the form (layer, neuron) -> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contains a phrase where a specific term, concept, or mechanism is introduced and then immediately defined or referred to using the term \"called.\" In other words, the attention head is activated when there is a direct association between a previously mentioned item and the term \"called.\" \\n\\nIn the inactive examples, the term \"called\" is often preceded by an additional descriptor (e.g., \"also called\" or \"sometimes called\"), or the association between the preceding concept and the term \"called\" is not as direct or immediate. This difference in structure or phrasing may be the reason why the attention head remains inactive in these cases.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nh_to_exp[(31, 988, 558)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import activating_dataset, prompt_generators\n",
    "from importlib import reload\n",
    "reload(activating_dataset)\n",
    "reload(prompt_generators)\n",
    "# Load Dataset\n",
    "from activating_dataset import ActivatingDataset\n",
    "from prompt_generators import GenerationPrompt\n",
    "\n",
    "all_neuron_heads = list(nh_to_exp.keys())\n",
    "\n",
    "eg_generation_prompts = {}\n",
    "\n",
    "for neuron_head in all_neuron_heads:\n",
    "    explanation_str = nh_to_exp[neuron_head].replace(\"\\n\", \"\")\n",
    "    token = neuron_to_token[(neuron_head[0], neuron_head[1])]\n",
    "    prompt_gen = GenerationPrompt(explanation_str, token)\n",
    "    gpt_4_prompt = prompt_gen.get_prompt(num_examples=10, generate_negative=True)\n",
    "    eg_generation_prompts[neuron_head] = gpt_4_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are studying attention heads in a transformer architecture neural network. Each attention head looks for some particular thing in a short document.\n",
      "The attention head being studied helps to predict that the next token is \" only\", but it is only active in some documents and not others.\n",
      "Explanation: \"\"\"\n",
      "In particular, this attention head is active when the document contains a statement or fact that is limited or restricted by the term \"only\". In the active examples, the attention head is highlighting a specific context or scenario where something is exclusive or unique. For example, \"Excersire can only work if you are following a proper diet\" implies that exercise alone is not enough.In the inactive examples, the term \"only\" is often used to emphasize the sole purpose or aspect of something rather than indicating exclusivity or a limitation in a broader context. For example, \"for one reason and one reason only\" uses \"only\" to stress that there is just one reason, but it doesn't indicate a unique or restrictive situation that the attention head would look for.\n",
      "\"\"\"\n",
      "With this explanation, generate 10 examples of documents that activate this attention head, and 10 examples of documents that do not activate this attention head.\n",
      "The documents should be 2 sentences long, and the token \" only\" should appear in the second sentence.\n",
      "\n",
      "Desired Format:\n",
      "Examples where the attention head is active:\n",
      "1. <example_1>\n",
      "2. <example_2>\n",
      "...\n",
      "\\Examples where the attention head is inactive:\n",
      "1. <example_1>\n",
      "2. <example_2>\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eg_generation_prompts[(31, 3621, 468)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GPT-4 to generate examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "filename = \"data/eg_generation_1.jsonl\"\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    raise Exception(\"File already exists!\")\n",
    "\n",
    "jobs = [\n",
    "            {\"model\":\"gpt-4\",\n",
    "            \"messages\":[{\"role\": \"user\", \"content\": gpt_4_prompt}],\n",
    "            \"max_tokens\":800, \n",
    "        } for gpt_4_prompt in eg_generation_prompts.values()]\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for job in jobs:\n",
    "        json_string = json.dumps(job)\n",
    "        f.write(json_string + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "now = int(time.time())\n",
    "print(now)\n",
    "\n",
    "#########################################################################################\n",
    "# WHEN YOU UPDATE THE CELL, REMEMBER TO UPDATE THE JSONL FILE NAME IF YOU'VE CHANGED IT #\n",
    "#########################################################################################\n",
    "current_time = 1686792857\n",
    "\n",
    "if current_time + 20 < now: # Sanity check to make sure you don't spam this cell\n",
    "    raise Exception(\"Update the current_time variable to be able to run this cell! Copy and paste the number above.\")\n",
    "else:\n",
    "    print(\"all gucci\")\n",
    "    !python3 api_request_parallel_processor.py --requests_filepath data/eg_generation_1.jsonl --request_url https://api.openai.com/v1/chat/completions --max_requests_per_minute 100 --max_tokens_per_minute 20000\n",
    "    # ^ This is very scary because the stdout looks like it sends repeated requests for the same thing so just run it in terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results and make the {neuron_head: response} dictionary\n",
    "\n",
    "with open('data/eg_generation_1_results.jsonl', 'r') as json_file:\n",
    "    raw_explanations = list(json_file)\n",
    "\n",
    "prompt_to_response = {}\n",
    "neuron_head_to_response = {}\n",
    "neuron_head_to_prompt = eg_generation_prompts\n",
    "neuron_heads = list(neuron_head_to_prompt.keys())\n",
    "\n",
    "for i, json_str in enumerate(raw_explanations):\n",
    "    result = json.loads(json_str)\n",
    "    prompt = result[0][\"messages\"][0][\"content\"]\n",
    "    response = result[1][\"choices\"][0][\"message\"][\"content\"]\n",
    "    prompt_to_response[prompt] = response\n",
    "\n",
    "for neuron_head in neuron_heads:\n",
    "    prompt = neuron_head_to_prompt[neuron_head]\n",
    "    response = prompt_to_response[prompt]\n",
    "    neuron_head_to_response[neuron_head] = response\n",
    "\n",
    "with open(\"data/eg_generation_1_nh_to_exp.json\", \"w\") as f:\n",
    "    nh_to_response_str_key = {str(k):v for k,v in neuron_head_to_response.items()}\n",
    "    json.dump(nh_to_response_str_key, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prompts For Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import reload from wherever\n",
    "from importlib import reload\n",
    "import prompt_generators\n",
    "reload(prompt_generators)\n",
    "from prompt_generators import IterationPromptGen\n",
    "from neuron_functions import get_head_attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/eg_generation_1_nh_to_exp.json\") as f:\n",
    "    generated_examples_str_key = json.load(f)\n",
    "    generated_examples = {tuple(int(value.strip()) for value in k.strip('()').split(',')): v for k, v in generated_examples_str_key.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:09,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:14,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:16,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:20,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:25,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:30,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:33,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:35,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:39,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:43,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:47,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:53,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:59,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [01:03,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [01:04,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [01:06,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [01:10,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [01:15,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [01:17,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [01:19,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [01:22,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [01:27,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [01:31,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [01:34,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [01:37,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [01:42,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [01:45,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [01:47,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [01:49,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [01:54,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [01:58,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [01:59,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [02:01,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [02:02,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [02:04,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [02:08,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [02:10,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [02:15,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [02:17,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [02:18,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [02:22,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [02:28,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [02:30,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [02:32,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [02:33,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [02:35,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [02:36,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [02:38,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [02:39,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [02:40,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [02:48,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [02:59,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [03:03,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [03:04,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [03:09,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [03:12,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [03:14,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [03:19,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [03:22,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [03:23,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [03:24,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [03:26,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [03:27,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [03:29,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [03:33,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [03:36,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [03:42,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [03:44,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [03:47,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [03:51,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [03:54,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [03:56,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [03:58,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [03:59,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [04:02,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [04:07,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [04:09,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [04:12,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [04:18,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [04:21,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82it [04:25,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [04:29,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [04:31,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [04:34,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [04:37,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87it [04:38,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [04:43,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [04:47,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [04:48,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [04:52,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [04:54,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93it [04:57,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [05:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [05:01,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [05:04,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97it [05:07,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [05:11,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [05:16,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [05:19,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [05:20,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [05:22,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [05:26,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [05:28,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [05:30,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106it [05:32,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "107it [05:37,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [05:40,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109it [05:42,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [05:43,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [05:44,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [05:45,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [05:50,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114it [05:53,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115it [05:58,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116it [06:01,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117it [06:03,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [06:09,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119it [06:12,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [06:13,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [06:18,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "122it [06:21,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [06:23,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124it [06:24,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [06:29,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126it [06:36,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [06:43,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128it [06:49,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [06:55,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [07:02,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "131it [07:09,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "132it [07:15,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "133it [07:18,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [07:20,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "135it [07:22,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "136it [07:24,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "137it [07:26,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138it [07:29,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "139it [07:34,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [07:38,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141it [07:44,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "142it [07:48,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "143it [07:50,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [07:54,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [07:59,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146it [08:01,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "147it [08:07,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [08:10,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "149it [08:15,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [08:20,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "151it [08:23,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [08:30,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "153it [08:36,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "154it [08:38,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "155it [08:40,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [08:41,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [08:45,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158it [08:47,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159it [08:49,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [08:50,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [08:51,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "162it [08:55,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "163it [08:57,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [08:59,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [09:05,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [09:13,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168it [09:18,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169it [09:22,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170it [09:27,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "171it [09:33,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172it [09:36,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [09:37,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "174it [09:38,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "175it [09:40,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [09:41,  3.31s/it]\n"
     ]
    }
   ],
   "source": [
    "check_if_responses_can_be_processed = False # Do this first\n",
    "iteration_prompts = {}\n",
    "iteration_prompts_ref = {}\n",
    "\n",
    "for i, (neuron_head, generated_example) in tqdm(enumerate(generated_examples.items())):\n",
    "    neuron, head_no = (neuron_head[0], neuron_head[1]), neuron_head[2]\n",
    "    relevant_token = neuron_to_token[neuron]\n",
    "\n",
    "    capitalised_token = relevant_token[0] + relevant_token[1].capitalize() + relevant_token[2:] # we can do this because by definition we got \" word\" tokens\n",
    "    \n",
    "    if 'is_inactive' in generated_example:\n",
    "        generated_example = generated_example.replace('is_inactive', 'is inactive') # For some reason this was in one prompt\n",
    "\n",
    "    # Extract the positive and negative examples. \n",
    "    def get_list_of_strings(string):\n",
    "        return [s[3:] if s[1] == \".\" else s[4:] for s in string.split(\"\\n\") if s != ''] # Wonky but it works\n",
    "\n",
    "    positive_examples = generated_example.split(\"Examples where the attention head is active:\")[1].split(\"Examples where the attention head is inactive:\")[0]\n",
    "    negative_examples = generated_example.split(\"Examples where the attention head is inactive:\")[1]\n",
    "    positive_examples = get_list_of_strings(positive_examples)\n",
    "    negative_examples = get_list_of_strings(negative_examples)\n",
    "\n",
    "    # Cut off the examples at the last instance of <token>. They may happen more than once.\n",
    "    positive_examples = [eg[:max(eg.rfind(relevant_token), eg.rfind(capitalised_token))] for eg in positive_examples]\n",
    "    negative_examples = [eg[:max(eg.rfind(relevant_token), eg.rfind(capitalised_token))] for eg in negative_examples]\n",
    "\n",
    "    if not check_if_responses_can_be_processed:\n",
    "        # Load and Truncate Prompts (slightly cursed way of string -> token -> truncated tokens -> string)\n",
    "        trunc_prompts = positive_examples + negative_examples\n",
    "        # Run head attribution\n",
    "        tokens = model.to_tokens(trunc_prompts, prepend_bos=True).to(device=device)\n",
    "        original_logits, cache = model.run_with_cache(tokens, )\n",
    "\n",
    "        # # Prepare prompts by heads\n",
    "        head_attribution = get_head_attribution(model, cache, tokens, neuron)\n",
    "        _, top_heads = torch.topk(head_attribution, k=3, dim=-1)\n",
    "        top_heads_list = top_heads.tolist()\n",
    "\n",
    "        top_heads_positive = top_heads_list[:len(positive_examples)]\n",
    "        top_heads_negative = top_heads_list[len(positive_examples):]\n",
    "\n",
    "        positive_examples_correct = []\n",
    "        negative_examples_correct = []\n",
    "\n",
    "        for i, example in enumerate(positive_examples):\n",
    "            if head_no in top_heads_positive[i]:\n",
    "                positive_examples_correct.append(example)\n",
    "            else:\n",
    "                negative_examples_correct.append(example)\n",
    "\n",
    "        for i, example in enumerate(negative_examples):\n",
    "            if head_no in top_heads_negative[i]:\n",
    "                positive_examples_correct.append(example)\n",
    "            else:\n",
    "                negative_examples_correct.append(example)\n",
    "\n",
    "        explanation_str = nh_to_exp[neuron_head].replace(\"\\n\", \"\")\n",
    "        iter_gen = IterationPromptGen(positive_examples, negative_examples, positive_examples_correct, negative_examples_correct, relevant_token, explanation_str)\n",
    "        iteration_prompt = iter_gen.get_prompt()\n",
    "\n",
    "        iteration_prompts[neuron_head] = iteration_prompt\n",
    "        iteration_prompts_ref[neuron_head] = [positive_examples, negative_examples, positive_examples_correct, negative_examples_correct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following solutions are the output of a Bayesian reasoner which is optimized to explain the function of attention heads in a neural network using limited evidence. Each attention head looks for some particular thing in a short passage.\n",
      "The reasoner is trying to revise the explanation for an attention head that helps the model to predict that the next token is \" only\".\n",
      "The current explanation is: This attention head is active when the document <explanation>contains a statement or fact that is limited or restricted by the term \"only\". In the active examples, the attention head is highlighting a specific context or scenario where something is exclusive or unique. For example, \"Excersire can only work if you are following a proper diet\" implies that exercise alone is not enough.In the inactive examples, the term \"only\" is often used to emphasize the sole purpose or aspect of something rather than indicating exclusivity or a limitation in a broader context. For example, \"for one reason and one reason only\" uses \"only\" to stress that there is just one reason, but it doesn't indicate a unique or restrictive situation that the attention head would look for.</explanation>\n",
      "With the explanation, the reasoner categorises the following examples of documents that activate this attention head:\n",
      "Examples where the attention head is active: \"\"\"\n",
      "*Sarah wanted to buy a blue dress. Unfortunately, the store had only\n",
      "*Jack tried to fix the broken machine. However, it would operate only\n",
      "*Many people believe that studying hard ensures success. Education is essential, but networking only\n",
      "*I love eating ice cream in the summer. This ice cream shop has only\n",
      "*The new software update is designed to improve efficiency. Regrettably, it is only\n",
      "*Lisa was hoping to dine at her favorite restaurant. Unfortunately, it serves dinner only\n",
      "*The plants in the garden require daily care. Watering them only\n",
      "*To unlock the secret level, you must collect all the hidden keys. The door will open only\n",
      "*The seminar was informative but left many attendees with questions. The speaker took questions only\n",
      "*Our office has a policy of providing sick leave. Sick leaves are granted only\n",
      "\"\"\"\n",
      "Examples where the attention head is inactive: \"\"\"\n",
      "*There was one single reason I took up this challenge. It was to prove to myself, and myself only\n",
      "*Sarah had one goal and one goal only\n",
      "*He decided to show up at my doorstep on that rainy night. He had his love, and his love only\n",
      "*The organization focused on one objective. They aimed to provide clean water and that only\n",
      "*She had one dream while growing up. She wanted to become an astronaut and only\n",
      "*This new recipe contains one unique ingredient. It uses ground cinnamon and only\n",
      "*The boss set a clear expectation for the team. They were to focus on creativity and creativity only\n",
      "*This app serves a single purpose. It is designed for tracking expenses and only\n",
      "*The bakery only offers one dessert per day. Today, they had chocolate cake and that only\n",
      "*The conference focused on one topic only\n",
      "\"\"\"\n",
      "\n",
      "The reasoner receives the following new evidence. The examples were ran through the model to determine whether the attention head was active or not. Here are the correct categories for each example:\n",
      "Examples where the attention head is active: \"\"\"\n",
      "*\n",
      "\"\"\"\n",
      "Examples where the attention head is inactive: \"\"\"\n",
      "*Sarah wanted to buy a blue dress. Unfortunately, the store had only\n",
      "*Jack tried to fix the broken machine. However, it would operate only\n",
      "*Many people believe that studying hard ensures success. Education is essential, but networking only\n",
      "*I love eating ice cream in the summer. This ice cream shop has only\n",
      "*The new software update is designed to improve efficiency. Regrettably, it is only\n",
      "*Lisa was hoping to dine at her favorite restaurant. Unfortunately, it serves dinner only\n",
      "*The plants in the garden require daily care. Watering them only\n",
      "*To unlock the secret level, you must collect all the hidden keys. The door will open only\n",
      "*The seminar was informative but left many attendees with questions. The speaker took questions only\n",
      "*Our office has a policy of providing sick leave. Sick leaves are granted only\n",
      "*There was one single reason I took up this challenge. It was to prove to myself, and myself only\n",
      "*Sarah had one goal and one goal only\n",
      "*He decided to show up at my doorstep on that rainy night. He had his love, and his love only\n",
      "*The organization focused on one objective. They aimed to provide clean water and that only\n",
      "*She had one dream while growing up. She wanted to become an astronaut and only\n",
      "*This new recipe contains one unique ingredient. It uses ground cinnamon and only\n",
      "*The boss set a clear expectation for the team. They were to focus on creativity and creativity only\n",
      "*This app serves a single purpose. It is designed for tracking expenses and only\n",
      "*The bakery only offers one dessert per day. Today, they had chocolate cake and that only\n",
      "*The conference focused on one topic only\n",
      "\"\"\"\n",
      "\n",
      "In light of the new evidence, the reasoner revises the current explanation to: This attention head is active when the document <explanation>\n"
     ]
    }
   ],
   "source": [
    "print(iteration_prompts[(31, 3621, 468)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/iteration_prompts_1.json\", \"w\") as f:\n",
    "    iteration_prompts_str_key = {str(k):v for k,v in iteration_prompts.items()}\n",
    "    json.dump(iteration_prompts_str_key, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/iteration_prompts_ref_1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(iteration_prompts_ref, f) \n",
    "\n",
    "# ref is original, ref_1 is with token at end of example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4059659090909092\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175
         ],
         "xaxis": "x",
         "y": [
          0,
          0,
          1,
          0.4,
          0.4,
          0.1,
          0.9,
          0.2,
          0,
          0,
          0,
          0,
          0,
          0.8,
          0.5,
          0.1,
          0.9,
          0.6,
          0.6,
          0.6,
          0,
          0.6,
          0.2,
          0,
          0,
          0.2,
          0.3,
          0.2,
          0.3,
          0.1,
          0.6,
          0.1,
          0.9,
          0.7,
          0.6,
          0.7,
          0.1,
          0.5,
          0.6,
          0.1,
          0.7,
          1,
          0.7,
          0.6,
          0.7,
          0.8,
          1,
          1,
          0.3,
          0,
          0,
          1,
          1,
          0,
          1,
          0.8,
          0.8,
          0.4,
          0.4,
          0.3,
          0.7,
          0.9,
          0.5,
          0.8,
          0.9,
          0,
          0.4,
          0,
          0.6,
          0.9,
          0.4,
          0.2,
          0.5,
          0.6,
          0.3,
          0,
          0.5,
          0,
          0.1,
          0.4,
          0.3,
          0,
          0.3,
          0,
          1,
          0,
          0,
          0.2,
          0.4,
          0.5,
          0.6,
          0.5,
          0.3,
          1,
          0.9,
          0.6,
          0.3,
          0,
          0.1,
          0.1,
          0.4,
          0.3,
          0,
          0.4,
          0,
          0.7,
          0,
          0.2,
          0,
          0.5,
          0.7,
          0,
          0,
          0,
          0.3,
          1,
          0,
          0,
          0.6,
          0.9,
          0.7,
          0,
          0,
          0.9,
          0,
          0,
          0,
          0.9,
          0.1,
          0,
          1,
          0.9,
          0.8,
          0,
          1,
          0.5,
          0,
          0,
          0.7,
          0.2,
          1,
          0,
          0,
          0,
          1,
          0.2,
          0.4,
          0.2,
          0.3,
          1,
          0,
          0.2,
          0.6,
          0.2,
          0.7,
          0,
          0.6,
          0,
          1,
          1,
          1,
          0.3,
          1,
          0.7,
          0.4,
          0,
          0.1,
          1,
          1,
          0.5,
          0.45,
          0,
          0,
          0,
          0.3,
          0.4
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175
         ],
         "xaxis": "x",
         "y": [
          0,
          0,
          10,
          6,
          13,
          1,
          9,
          8,
          0,
          0,
          0,
          0,
          0,
          9,
          7,
          3,
          9,
          6,
          6,
          16,
          1,
          6,
          2,
          0,
          2,
          7,
          5,
          2,
          9,
          1,
          6,
          1,
          9,
          8,
          6,
          10,
          3,
          7,
          9,
          1,
          9,
          19,
          13,
          8,
          7,
          14,
          20,
          17,
          4,
          0,
          0,
          10,
          10,
          0,
          19,
          15,
          16,
          4,
          7,
          6,
          8,
          16,
          9,
          11,
          16,
          1,
          6,
          0,
          8,
          15,
          4,
          2,
          7,
          6,
          5,
          0,
          5,
          0,
          1,
          12,
          3,
          0,
          5,
          0,
          20,
          0,
          0,
          3,
          6,
          7,
          8,
          5,
          3,
          20,
          17,
          6,
          3,
          0,
          1,
          1,
          7,
          10,
          1,
          5,
          0,
          13,
          0,
          8,
          0,
          14,
          14,
          0,
          0,
          0,
          3,
          19,
          0,
          0,
          6,
          12,
          8,
          2,
          0,
          10,
          0,
          0,
          0,
          14,
          1,
          0,
          12,
          11,
          8,
          0,
          18,
          11,
          0,
          0,
          8,
          6,
          20,
          0,
          0,
          0,
          10,
          3,
          4,
          12,
          10,
          15,
          0,
          2,
          6,
          2,
          7,
          0,
          6,
          0,
          20,
          10,
          19,
          5,
          10,
          17,
          6,
          0,
          1,
          20,
          20,
          9,
          17,
          0,
          0,
          1,
          5,
          9
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "positive_lengths = []\n",
    "accuracies = []\n",
    "for head, (a, b, c, d) in iteration_prompts_ref.items():\n",
    "    correct_positive = 0\n",
    "    for eg in a:\n",
    "        if eg in c:\n",
    "            correct_positive += 1\n",
    "    accuracies.append(correct_positive/len(a))\n",
    "    positive_lengths.append(len(c))\n",
    "\n",
    "print(sum(accuracies)/len(accuracies))\n",
    "scatter(np.arange(len(accuracies)), accuracies)\n",
    "scatter(np.arange(len(positive_lengths)), positive_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following solutions are the output of a Bayesian reasoner which is optimized to explain the function of attention heads in a neural network using limited evidence. Each attention head looks for some particular thing in a short passage.\n",
      "The reasoner is trying to revise the explanation for an attention head that helps the model to predict that the next token is \" together\".\n",
      "The current explanation is: This attention head is active when the document <explanation>mentions the concept of things or elements coming together or being brought together, but not in cases where the phrase \"taken together\" is used or where the sense of combining is not directly expressed. It seems to be sensitive to the specific way in which the idea of joining or merging is conveyed in the text, and not just the general notion of togetherness.</explanation>\n",
      "With the explanation, the reasoner categorises the following examples of documents that activate this attention head:\n",
      "Examples where the attention head is active: \"\"\"\n",
      "*The new project requires collaboration from several departments. They will all work together\n",
      "*The recipe calls for mixing the wet and dry ingredients separately. Then, you should combine them together\n",
      "*Our team is brainstorming ideas for the marketing campaign. We plan to incorporate everyone's ideas together\n",
      "*The couple decided to start a business and lead a fulfilling life. They believed that pursuing their dreams together\n",
      "*The engineer created a device capable of integrating multiple functions into one. This innovation brought temperature, humidity, and air quality sensors together\n",
      "*The members of the committee had different ideas about addressing the issue. After some discussion, they agreed to draft a solution together\n",
      "*The musicians decided to form a band to make beautiful music. They found that their voices harmonized well together\n",
      "*The charity event was the result of the diligent efforts of numerous volunteers. They all came together\n",
      "*The family was looking forward to their reunion after several years apart. They were excited about creating new memories together\n",
      "*The science conference featured experts from various fields sharing their research. The speakers discussed how to tackle major challenges together\n",
      "\"\"\"\n",
      "Examples where the attention head is inactive: \"\"\"\n",
      "*The investors looked at the quarterly report and analyzed each section. When taken together\n",
      "*The siblings had a series of disagreements and decided to take a break from each other. It was a rare occasion to see them sitting together\n",
      "*The two friends had a pact to never let anything get in their way. They had withstood many trials and ended up stronger together\n",
      "*The author completed the first draft of her novel after years of hard work. She felt a sense of accomplishment when reading it together\n",
      "*The teacher assigned several readings for the week. To understand the main topic, the students needed to study the materials and connect them together\n",
      "*The marathon participants queued up at the starting line, waiting for the race to begin. They were all in that moment together\n",
      "*The team of researchers reviewed the previous studies on the subject. The meta-analysis, which looked at the findings together\n",
      "*The employee had a lot on her plate, with multiple tasks to complete. She felt overwhelmed trying to address all the responsibilities together\n",
      "*The traveler looked out into the vast landscape, capturing the natural beauty in a picture. The elements of the scene captured together\n",
      "*The artist painted a masterpiece that depicted a bustling market scene. Even though the people were together\n",
      "\"\"\"\n",
      "\n",
      "The reasoner receives the following new evidence. The examples were ran through the model to determine whether the attention head was active or not. Here are the correct categories for each example:\n",
      "Examples where the attention head is active: \"\"\"\n",
      "*The new project requires collaboration from several departments. They will all work together\n",
      "*The couple decided to start a business and lead a fulfilling life. They believed that pursuing their dreams together\n",
      "*The musicians decided to form a band to make beautiful music. They found that their voices harmonized well together\n",
      "*The charity event was the result of the diligent efforts of numerous volunteers. They all came together\n",
      "*The family was looking forward to their reunion after several years apart. They were excited about creating new memories together\n",
      "*The two friends had a pact to never let anything get in their way. They had withstood many trials and ended up stronger together\n",
      "*The marathon participants queued up at the starting line, waiting for the race to begin. They were all in that moment together\n",
      "\"\"\"\n",
      "Examples where the attention head is inactive: \"\"\"\n",
      "*The recipe calls for mixing the wet and dry ingredients separately. Then, you should combine them together\n",
      "*Our team is brainstorming ideas for the marketing campaign. We plan to incorporate everyone's ideas together\n",
      "*The engineer created a device capable of integrating multiple functions into one. This innovation brought temperature, humidity, and air quality sensors together\n",
      "*The members of the committee had different ideas about addressing the issue. After some discussion, they agreed to draft a solution together\n",
      "*The science conference featured experts from various fields sharing their research. The speakers discussed how to tackle major challenges together\n",
      "*The investors looked at the quarterly report and analyzed each section. When taken together\n",
      "*The siblings had a series of disagreements and decided to take a break from each other. It was a rare occasion to see them sitting together\n",
      "*The author completed the first draft of her novel after years of hard work. She felt a sense of accomplishment when reading it together\n",
      "*The teacher assigned several readings for the week. To understand the main topic, the students needed to study the materials and connect them together\n",
      "*The team of researchers reviewed the previous studies on the subject. The meta-analysis, which looked at the findings together\n",
      "*The employee had a lot on her plate, with multiple tasks to complete. She felt overwhelmed trying to address all the responsibilities together\n",
      "*The traveler looked out into the vast landscape, capturing the natural beauty in a picture. The elements of the scene captured together\n",
      "*The artist painted a masterpiece that depicted a bustling market scene. Even though the people were together\n",
      "\"\"\"\n",
      "\n",
      "In light of the new evidence, the reasoner revises the current explanation to: This attention head is active when the document <explanation>\n"
     ]
    }
   ],
   "source": [
    "print(list(iteration_prompts.values())[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "filename = \"data/iteration_prompt_1.jsonl\"\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    raise Exception(\"File already exists!\")\n",
    "\n",
    "jobs = [\n",
    "            {\"model\":\"gpt-4\",\n",
    "            \"messages\":[{\"role\": \"user\", \"content\": gpt_4_prompt}],\n",
    "            \"max_tokens\":200, \n",
    "        } for gpt_4_prompt in iteration_prompts.values()]\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for job in jobs:\n",
    "        json_string = json.dumps(job)\n",
    "        f.write(json_string + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "now = int(time.time())\n",
    "print(now)\n",
    "\n",
    "#########################################################################################\n",
    "# WHEN YOU UPDATE THE CELL, REMEMBER TO UPDATE THE JSONL FILE NAME IF YOU'VE CHANGED IT #\n",
    "#########################################################################################\n",
    "current_time = 1686792857\n",
    "\n",
    "if current_time + 20 < now: # Sanity check to make sure you don't spam this cell\n",
    "    raise Exception(\"Update the current_time variable to be able to run this cell! Copy and paste the number above.\")\n",
    "else:\n",
    "    print(\"all gucci\")\n",
    "    !python3 api_request_parallel_processor.py --requests_filepath data/iteration_prompt_1.jsonl --request_url https://api.openai.com/v1/chat/completions --max_requests_per_minute 100 --max_tokens_per_minute 20000\n",
    "    # ^ This is very scary because the stdout looks like it sends repeated requests for the same thing so just run it in terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results and make the {neuron_head: response} dictionary\n",
    "\n",
    "with open('data/iteration_prompt_1_results.jsonl', 'r') as json_file:\n",
    "    raw_explanations = list(json_file)\n",
    "\n",
    "prompt_to_response = {}\n",
    "neuron_head_to_response = {}\n",
    "neuron_head_to_prompt = iteration_prompts\n",
    "neuron_heads = list(neuron_head_to_prompt.keys())\n",
    "\n",
    "for i, json_str in enumerate(raw_explanations):\n",
    "    result = json.loads(json_str)\n",
    "    prompt = result[0][\"messages\"][0][\"content\"]\n",
    "    response = result[1][\"choices\"][0][\"message\"][\"content\"]\n",
    "    prompt_to_response[prompt] = response\n",
    "\n",
    "for neuron_head in neuron_heads:\n",
    "    prompt = neuron_head_to_prompt[neuron_head]\n",
    "    response = prompt_to_response[prompt]\n",
    "    neuron_head_to_response[neuron_head] = response\n",
    "\n",
    "\n",
    "with open(\"data/iteration_prompt_1_nh_to_exp.json\", \"w\") as f:\n",
    "    nh_to_response_str_key = {str(k):v for k,v in neuron_head_to_response.items()}\n",
    "    json.dump(nh_to_response_str_key, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains a statement or fact where the term \"only\" is used to emphasize exclusivity, limitation, or a specific condition. In the active examples, the attention head is highlighting scenarios where something is restricted or unique due to the presence of the term \"only\". For example, \"To unlock the secret level, you must collect all the hidden keys. The door will open only\" implies that meeting a particular condition is required for the door to open. In the inactive examples, the term \"only\" is often used to emphasize the single aspect, purpose, or focus of something without necessarily indicating a broader context restriction or exclusivity. For example, \"Sarah had one goal and one goal only\" uses \"only\" to stress that there is just one goal but doesn't indicate a unique situation that the attention head looks for.\n",
      "contains a statement or fact where the term \"only\" is used to emphasize exclusivity, limitation, or a specific condition. In the active examples, the attention head is highlighting scenarios where something is restricted or unique due to the presence of the term \"only\". For example, \"To unlock the secret level, you must collect all the hidden keys. The door will open only\" implies that meeting a particular condition is required for the door to open. In the inactive examples, the term \"only\" is often used to emphasize the single aspect, purpose, or focus of something without necessarily indicating a broader context restriction or exclusivity. For example, \"Sarah had one goal and one goal only\" uses \"only\" to stress that there is just one goal but doesn't indicate a unique situation that the attention head looks for.\n",
      "contains a phrase that describes a specific condition, restriction, or purpose before the word \"only.\" In active examples, we see phrases like \"for informational purposes only,\" \"for authorized users only,\" \"for low-income families only,\" or \"on prescription only.\" These phrases establish a particular context or criteria that leads to the appearance of the word \"only\" at the end. On the other hand, in inactive examples, the word \"only\" is used in different contexts, without a clear specific condition, restriction, or purpose preceding it.\n",
      "contains the word \"only\" in a context where it emphasizes exclusivity, limitations, or a singular focus. In the active examples, \"only\" is found in situations where it specifies a restriction or a particular aspect, such as \"for personal usage only\" or \"allowed to wear only\". In the inactive examples, even though the word \"only\" is present, it is not used to convey the same emphasis on exclusivity or limitations, or it is in a context where it is a part of a broader description or list. The context of \"only\" in inactive examples is often related to choices, options, or general conditions (e.g., \"includes premium features only\" or \"focusing on renewable energy only\") without directly specifying a restriction or a singular focus on purpose or reason.\n",
      "contains a phrase indicating a specific purpose, intention, or expectation, which could be followed by the word \"only\". In the active examples, the attention head recognizes patterns such as \"for...purposes only\", \"meant for awareness purposes only\", or \"the deal is only\", highlighting a limitation, restriction, or particular focus on the information or content provided. In the inactive examples, the word \"only\" is either used differently in the context, such as \"known only\", or it is used in the presence of contrasting and non-restrictive patterns and phrases, making it difficult for the attention head to identify a distinct pattern.\n",
      "contains a context or phrase that is related to record-breaking, chart-topping, or being highly ranked in terms of popularity or achievement, often in the music domain. The attention head is inactive when the document contains numbers related to rankings, positions, or sequences in other domains, as well as when it involves identification, phone numbers, parts, approvals, or other non-ranking contexts. The active examples usually involve quantifying the success or popularity of a subject, entity, or performance.\n",
      "discusses a specific identification, contact, or reference number that is directly associated with an object, person, organization, or the process of identifying or contacting something or someone. In the active examples, the attention head detects scenarios involving tax identification numbers, phone numbers, IMEI numbers, mobile numbers, serial numbers, order numbers, parcel numbers, and government-issued ID numbers. The attention head remains inactive on documents discussing passport numbers, rankings, standings, art pieces or exhibits, bib numbers, racing car numbers, science fair projects, crossword puzzle clues, podcast episodes, tracking numbers for animals, and labeled art pieces, as these instances do not directly involve identifying or contacting a target in the same manner as the numbers in active examples.\n",
      "discusses numbers that serve a practical or functional purpose in a variety of contexts, such as emergency numbers, contact numbers, exhibit numbers, part numbers, patent application numbers, session numbers, and identification numbers. The attention head is not limited to rank-related or usage-specific numbers as previously described. Instead, it encompasses a broader range of numbers that facilitate the communication of relevant information, organization, or access in different scenarios. The attention head is inactive when the document focuses on numbers related to ranking, predictions, entertainment value, and personal achievements, or instances where the number serves a more symbolic or representational purpose.\n",
      "contains phrases with the word \"go\" used in a variety of contexts, such as actions, situations, directions, comparisons, or examples. The initial explanation focusing on comparative or descriptive expressions such as \"as far as,\" \"way to go,\" or \"let go\" was not able to accurately predict the attention head's activation. After analyzing the new evidence, it seems that the attention head is not specifically looking for these phrases but may be activated by a broader range of sentences where the word \"go\" is present, regardless of the context. The reason for the attention head's inconsistent activation in these examples still remains unclear and requires further investigation. The attention head could be focusing on another aspect of the sentences that is not immediately apparent, or it could be an uncertain case where the model is not able to accurately predict the next token as \"go.\"\n",
      "contains situations or contexts where a change, decision, or transition is occurring or about to occur, often involving the word \"go\" or similar actions. It seems to be less focused on the specific phrase \"come and go\" and more on the general concept of movement, progress, or change in various aspects of life, such as emotions, relationships, opportunities, and events. In both active and inactive examples, the word \"go\" can be present, but the attention head is more likely to be active when \"go\" is used in a context reflecting change, transition, or decision-making. In the inactive examples, the attention head is not active since the context it specializes in is not relevant or \"go\" is used in a more literal sense, such as physical movement or comparison.\n",
      "contains situations or contexts where a change, decision, or transition is occurring or about to occur, often involving the word \"go\" or similar actions. It seems to be less focused on the specific phrase \"come and go\" and more on the general concept of movement, progress, or change in various aspects of life, such as emotions, relationships, opportunities, and events. In both active and inactive examples, the word \"go\" can be present, but the attention head is more likely to be active when \"go\" is used in a context reflecting change, transition, or decision-making. In the inactive examples, the attention head is not active since the context it specializes in is not relevant or \"go\" is used in a more literal sense, such as physical movement or comparison.\n",
      "discusses actions, events, or situations that are anticipated, prepared for, or involve a sense of readiness, movement, or progress. In the active examples, the sentences tend to describe a process, a change, or an action that involves a person or group getting ready or feeling prepared to take on a challenge, embark on a journey, or face an event, such as \"After weeks of dedicated practice, she was finally ready to go,\" \"The rescue mission had been carefully planned. The team members were now ready to go,\" or \"On New Year's Eve, people made their resolutions. In high spirits, they were ready to go and achieve their goals.\" In the inactive examples, the use of \"go\" is more related to comparisons (e.g., \"as far as capital cities go\"), temporal changes (e.g., \"seems to come and go\"), or simple descriptions (e.g., \"on-the-go\") without strongly suggesting a sense of readiness, preparedness, or\n",
      "contains a variety of situations or contexts in which the phrase \"to go\" or \"the go\" is used. It appears that the attention head is not necessarily limited to more complex structures or additional information, as previously thought. Instead, the attention head seems to be sensitive to various scenarios, both simple and complex, where the concept of \"going\" or \"leaving\" is relevant. The attention head may be assisting the model in understanding the diverse ways in which \"go\" can be applied and used in different contexts to improve its prediction accuracy.\n",
      "contains phrases implying a combination, joining, or merging of two or more elements or ideas, particularly in the context of problem-solving or achieving a specific goal. In the active examples, we can see expressions like \"fit perfectly together\", \"fuse the best aspects together\", \"link the theories together\", and \"merge modern minimalism and rustic farmhouse together\", which suggest a process of combining or connecting things to meet an objective. Additionally, examples such as \"snapping the interlocking pieces together\" and \"bringing these distinct approaches together\" indicate tasks that involve assembling or integrating components.In the inactive examples, the word \"together\" is often used in a different context or doesn't play a central role in the action of combining things for a goal-oriented purpose. For instances, phrases like \"gathered together\", \"bringing people together\", \"taken together\", and \"bond together\" do not convey the same sense of merging or joining things as in the active examples. In some cases, \"together\n",
      "mentions people or groups working, collaborating, or experiencing something together, emphasizing a sense of unity, shared purpose, or camaraderie. However, it is not active when the concept of combining or merging objects, ideas, or elements is conveyed, even if the word \"together\" is used. This attention head seems to be more focused on the interpersonal aspect of togetherness, rather than on the process of combining or merging things.\n",
      "mentions a situation where distinct elements, whether they be objects, ideas, or narratives, are brought together to create a new unified whole, often in a context where the merging of these elements is central to the meaning or purpose of the passage. For example, the attention head is active in cases where separate stories or documentaries are viewed together for comparison or analysis, or when a company merges with another to form a new unified entity. In contrast, the head is inactive when \"together\" is used in situations where the merging of elements, such as mixing ingredients together in a recipe, is not the main focus of the text or when phrases like \"taken together\" simply serve as a linguistic device for connecting pieces of information.\n",
      "ends with the phrase \"Taken together\" and is focused on a summary or conclusion drawn from multiple sources, studies, or perspectives. It seems to be specifically looking for this phrase at the end of a document to predict the last token \"together.\" In the inactive examples, even though the word \"together\" appears in various contexts, it is not preceded by the word \"Taken,\" and thus the attention head remains inactive. The attention head is sensitive to the specific context of \"Taken together\" and does not activate for other variations or uses of the word \"together.\"\n",
      "contains the phrase \"Taken together\" in various contexts, especially near the end of the document. It primarily looks for this phrase to predict the next token \"together.\" However, the attention head is not exclusively sensitive to \"Taken together\" and might sometimes remain inactive in its presence, as demonstrated by a few of the new examples. In the inactive examples, the word \"together\" appears in various contexts unrelated to the phrase \"Taken together,\" which does not activate the attention head. The attention head is sensitive to the specific context of \"Taken together\" and does not activate for other variations or uses of the word \"together.\"\n",
      "contains the phrase \"Taken together\" in the context of summarizing information from various sources or aspects of a subject. It seems to be specifically looking for this phrase in sentences related to research, studies, or analysis of multiple facts or aspects. In active examples, the attention head can predict the next token \"together\" accurately due to the presence of the phrase \"Taken together\" in such contexts. In the inactive examples, even though the word \"together\" appears in various contexts, the attention head remains inactive because the context does not involve the summarization of information from multiple sources. The attention head is sensitive to the specific context of \"Taken together\" and does not activate for other variations or uses of the word \"together.\"\n",
      "contains a phrase or concept that is being referred to or defined using the term \"called,\" regardless of whether it uses the structure \"also called\" or not. In other words, the attention head activates when \"called\" or \"also called\" is used to introduce, explain, or provide an alternative name for a term, concept, or phenomenon. Previously, it was thought that the use of \"also called\" did not trigger this attention head, but the new evidence shows that both structures activate the attention head in various contexts.\n",
      "contains a phrase where a specific term, concept, or mechanism is being introduced or highlighted, and the term \"called\" is used to provide a name or label to that concept. Additionally, the attention head may also activate when the text refers to a popular or widely accepted name associated with the concept. The activation of this attention head is not strictly limited to immediate or direct associations between the mentioned item and the term \"called.\" It may also respond to cases where additional descriptors are used (e.g., \"popularly called\" or \"also called\") or when phrases provide more context about the concept before mentioning its name.In the inactive examples, the attention head may not be activated because the term \"called\" is preceded by qualifiers such as \"sometimes called\" or \"occasionally called,\" or the association between the preceding concept and the term \"called\" is more indirect or less immediate. Additionally, it may remain inactive when there are other attention heads better suited for capturing the specific structure or phrasing\n",
      "contains a term, name, or concept followed by a comma and the word \"also\" or \"sometimes\" before the token \"called\". In these cases, it identifies an alternative name or term for a particular subject, often related to specific phenomena, objects, or species. In the inactive examples, the context surrounding the token \"called\" does not follow this pattern, and instead, \"called\" is used in different ways such as referring to the name of a book, show, painting, an action done by someone, or explaining a concept without the specific \"also called\" pattern. The attention head may not be active for certain \"also called\" cases that are not as strongly associated with the naming of phenomena, objects, or species.\n",
      "contains a name or description for something or someone that has a notable or unique characteristic, followed by a comma and the word \"also\" or a phrase like \"as he came to be\" or \"as it came to be,\" leading to the next word being \"called.\" The attention head is looking for instances where an alternate name or term is being introduced for a particular subject, concept, or thing, with a specific emphasis on the uniqueness or notable aspect of the subject.In the inactive examples, the head is not active because either \"called\" is not the last word, or the context doesn't contain an alternate name introduction or a distinct characteristic that highlights the importance of the subject. These examples may still contain similar phrasing or the word \"called,\" but the overall structure does not meet the specific pattern that activates this attention head.\n",
      "uses words or phrases that indicate an alternative name, term, or description is being provided before the token \"called.\" However, the presence of a direct attributive phrase preceding the token \"called\" does not guarantee the activation of this attention head, as seen in some new evidence. There may be other factors, such as sentence structure or context, that influence its activation. In inactive examples, the word \"called\" is often preceded by phrases such as \"what he called,\" \"what is sometimes called,\" \"a condition called,\" or \"sometimes called\" with additional information in between, which remains consistent with the previous explanation. The difference between active and inactive examples might not solely rely on the presence or absence of a direct attributive phrase, but also on other linguistic features or patterns that require further investigation to determine their exact contribution to the attention head's activation or inactivation.\n",
      "contains a term or phrase that is followed by or includes the token \"called\". It appears that this attention head is identifying the context where a subject is given an alternative or specific name, but the pattern of having a comma and the word \"also\" is not strictly necessary for activation. In the inactive examples, the usage of \"called\" may not be closely related to an alternative or specific name describing the subject, or the context may not follow the same pattern as the active examples.\n",
      "refers to a new or initial occurrence, discovery, or experience of a particular thing or event, as well as when the concept of prioritizing, ordering, or emphasizing is present. The attention head is often active in situations where \"first\" is associated with phrases like \"for the first\" or \"the first,\" indicating the novelty of the event or experience. It is also active when \"first\" is used to indicate prioritizing tasks, decision-making, or ranking. In contrast, the attention head is inactive when \"first\" is used as a standalone word without any connection to a specific event, experience, or context where priority or emphasis is not implied.\n",
      "contains phrases that indicate a sequence, priority, or initial action such as \"for the first\", \"in the first\", \"starting with the first\", or \"buy the most important items first.\" These phrases convey a sense of order or importance, which help the model predict the next token as \"first.\" However, the attention head is inactive when the document focuses on a novel or first-time occurrence, such as \"their breakthrough was announced in the first\" or \"the space agency will send their first.\" Additionally, this attention head remains inactive when the document uses phrases such as \"first things first\" or \"meeting for the first\" that do not signify an ordered sequence or priority.In summary, the attention head is activated by the context of sequence, priority, or initial actions, making it more likely to predict the last token as \"first\" in such situations.\n",
      "mentions an individual having a new or unique experience, often described as doing or trying something \"for the first\" time. The attention head focuses on the personal aspect of the experience, emphasizing an individual's emotions, decisions, or actions related to the situation. In the inactive examples, the word \"first\" is used in various contexts but does not convey a personal experience or a significant event happening for the first time to an individual. Instead, it may indicate the order, priority, or sequence of events.\n",
      "contains a reference to a change or contrast related to the past, which might involve a transition, adaptation, or a comparison. The attention head often activates when the documents mention phrases like \"used to,\" \"once was,\" or \"earlier work\" and when there is a clear shift from past to present. It also focuses on the instances where there are adaptations or modifications in situations, behaviors, or states over time, such as in the examples with Sarah's change in smoking habits and the growth of Tom's puppy.\n",
      "mentions a shift in trends, preferences, or usage of certain items or concepts over time, with the last token \"used\" indicating a reference to a previous state or practice. The attention head remains inactive in documents where the last token \"used\" is a part of a phrase implying adaptation or getting accustomed to something, as well as in documents that describe changes in context or situations without directly comparing past and present usage of a particular aspect.\n",
      "mentions a significant change or transition in the subject's state, role, or circumstances over time, particularly involving improvements or adaptations. It gets activated when the context indicates that something was different in the past compared to the present situation or when the subject has undergone substantial personal or professional growth. In the active examples, we can observe changes in career, abilities, accomplishments, or technological advancements. In the inactive examples, the focus is more on adjusting or adapting to the current state rather than a marked change in the subject's role or condition.\n",
      "mentions a change or difference in rules, policies, or norms, generally implying that the current state or condition is not as permissive or flexible as it was previously. The attention head is particularly sensitive to changes that result in limitations, restrictions, or challenges compared to a past state. In many of the active examples, the attention head focuses on phrases like \"wasn't as encouraged as it used,\" emphasizing that the current situation is less favorable than before. In contrast, the inactive examples often include the verb \"used\" in a context that describes adaptation, habituation, or personal changes, without necessarily implying a significant alteration in the larger environment or system.\n",
      "contains a context that implies a specific action, event, or outcome is expected or needs to be completed within a certain timeframe or deadline. It is often triggered by phrases that describe a time-bound process, task, or result and involves a sense of obligation, promptness, or urgency by the subject. The attention head seems to work better in organized activities or situations requiring timely action. In the inactive examples, the context is more about personal preferences, choices, or situations that do not necessarily involve a sense of urgency or an explicit timeframe for completion.\n",
      "contains a situation or action that requires completion, submission, or response within a specific time frame, deadline, or limited availability. The active examples focus on the urgency of addressing tasks, meeting deadlines, or taking advantage of opportunities within a certain time limit. The inactive examples, on the other hand, emphasize activities, events, or conditions without any explicit time constraint, often focusing on the content or context of situations instead.\n",
      "is focused on a specific goal, outcome, or requirement that needs to be met or accomplished within a certain time frame or context. It often applies to tasks or events with clear milestones or objectives that should be completed within a given constraint. In the inactive examples, the context is broader, and the focus is on a wider range of subjects, such as experiences, exploration, learning, or appreciation, which do not necessarily involve a specific time frame or constraint related to the concept of \"within.\"\n",
      "mentions a direction, a distance or a specific manner, typically using phrases like \"all the way,\" \"this way,\" or \"that way.\" It focuses on sentences that emphasize a particular way, process, or method. It is also active when the context discusses alternatives or potential changes, such as in \"either way,\" \"the other way,\" or \"both ways.\" In contrast, this attention head stays inactive for documents that use the concept of \"way\" mainly in a comparative, indecisive, or open-ended context, like in \"which way,\" \"each way,\" or \"no matter which way.\"\n",
      "contains phrases that end with \"way\" and have a more abstract or figurative meaning, rather than a literal meaning related to distance, direction, or a specific expression. This can include describing a method, approach, perspective, or even a unique or beautiful situation. In these instances, \"way\" often appears immediately after words like \"this,\" \"that,\" or \"either,\" but it can also function as part of a more general description. In the inactive examples, the word \"way\" is used more explicitly to denote distance, direction, or as part of a common idiom or expression. Essentially, the attention head is activated when \"way\" is used to represent a broader idea or abstraction, rather than for its literal meaning.\n",
      "contains phrases that use the word \"way\" to indicate a particular manner, method, or approach, or when it is used to reference an unspecified or metaphorical direction or path. The attention head is sensitive to phrases like \"this way,\" \"that way,\" \"another way,\" and \"their way.\" However, it can also be active in cases where the word \"way\" is used in negative constructions (e.g., \"in no way\") and when it is part of an expression describing an encounter or experience (e.g., \"on his way\"). This attention head is inactive when the word \"way\" is used to indicate distance (e.g., \"all the way\") or to compare two entities (e.g., \"exact same way\").\n",
      "contains phrases that indicate a change in situation, choice, or decision, often involving situations where an alternative way, a new way, or a decision must be made. The attention head also activates when discussing the most efficient or quickest way to achieve something. In the inactive examples, the term \"way\" is primarily used in contexts that emphasize a static, continuous, or pre-determined situation, such as scenic routes, existing paths, or artistic techniques, without highlighting a change or alternative option.\n",
      "contains the word \"way\" followed by a noun or an adjective and expressing a sense of determination, goal, or desire. It focuses on instances where the word \"way\" is used to describe a specific approach, method, or manner that one is seeking or searching for. On the other hand, this attention head is inactive in cases where \"way\" is used to denote a literal or physical direction, distance, or location, as well as when it is used to describe various methods or approaches without a sense of determination or goal-oriented perspective.\n",
      "contains phrases or situations related to uncovering, finding out, or revealing information, often preceded by a question or statement of curiosity, such as \"keep reading to find out,\" \"let's find out,\" \"discover,\" or \"turns out.\" The attention head may also be activated by phrases describing someone going out or carrying something out. In the inactive examples, these phrases or situations are either absent, have a different structure or meaning, or are part of sentences that do not imply uncovering or revealing information.\n",
      "features phrases or sentences that establish a sense of anticipation, curiosity, or suspense for the reader. These can include phrases like \"turns out,\" \"find out,\" and \"read on,\" but may also involve scenarios where the reader wants to discover the outcome of a situation. Such scenarios often involve tense, emotional, or surprising situations. The attention head helps the model predict the next token \"out\" in these contexts. In the inactive examples, the attention head remains inactive because these phrases or scenarios generating anticipation are either absent or placed differently within the document, thus not contributing to the prediction of the last token \"out.\"\n",
      "contains phrases like \"it turns out,\" \"as it turns out,\" \"read on to find out,\" \"to find out,\" \"the cake came out,\" \"he found out,\" \"he found out,\" \"it came out,\" \"she carried it out,\" or \"they ruled out\" that indicate a revelation, discovery or an outcome that is being explained or shared, in both positive and negative contexts. These phrases may appear at the end of sentences or paragraphs, leading to a conclusion or moment of understanding. The attention head seems to be sensitive to not only specific phrases but also the context in which they are used, focusing on situations where an explanation or action is imminent or has recently occurred.\n",
      "contains a phrase where \"too\" is used to emphasize an excessive degree of a particular action, quality, or contextual situation. In the active examples, phrases like \"pushing himself too\", \"added way too\", \"staying awake too\", \"practicing for too\", \"must have put in way too\", \"excessive speed was way too\", \"been to Italy too\", and \"liked mysteries too\" show that the attention head is focused on situations where the excessive degree or relevance of something is significant. In the inactive examples, the usage of \"too\" is either part of a larger expression (e.g., \"not to pack her bags too\"), not emphasizing the excessive degree of an action (e.g., \"takes discussions too\"), or is used to express agreement or similarity (e.g., \"she wore a matching outfit too\"). As a result, the attention head is not activated in those cases.\n",
      "contains a phrase indicating an excessive or extreme aspect, often in situations involving personal experiences, sensory experiences, or decisions. Examples include \"way too,\" \"much too,\" or \"simply too.\" In these instances, the attention head helps to predict the last token as \"too.\" This attention head is likely focusing on contextual patterns that suggest an emphasis on excessive amounts or actions. In the inactive examples, the term \"too\" is still present, but it may not be associated with an emphasized extreme or excess, and therefore the attention head remains inactive. The attention head may also be inactive in some cases where an excessive aspect is present, but the context is more focused on external factors or general observations, such as the economy or weather.\n",
      "contains a phrase or structure that indicates a limit, boundary, or caution being advised or considered, whether in a physical, emotional, or situational context. It recognizes scenarios where the concept of \"too\" plays a role in warning about possible negative outcomes or potential overstepping of limits. The attention head is not necessarily restricted to proximity to the end of the document, as previously thought.In the active examples, the attention head is triggered by phrases such as \"don't fill it too\", \"you don't want your paper to become too\", \"worried about becoming too\", \"not to push themselves too\", \"must not be allowed to become too\", \"not to stretch too\", \"try not to get too\", and \"you wouldn't want the cake to become too\", as well as \"it was too coordinated, drawing too\", \"there were usually too\", \"she realized she was being too\", \"it was difficult for Abby to make a decision, as there were too\",\n",
      "contains a context where \"will\" is used in relation to decision-making, planning, or future actions, regardless of whether it is used as an auxiliary verb to indicate future tense or to express probability. This includes situations such as individuals making decisions, groups strategizing or deliberating, and the implementation of policies or projects. This attention head also seems to be active in cases where \"will\" is used as part of an expression that does involve decision-making, planning, or probable actions in the future. As a result, the attention head is more universally active in predicting the token \"will\" in various contexts.\n",
      "mentions \"will\" in a context that implies an active choice, change, or decision-making process, whether it is related to human actions or natural events. In the active examples, \"will\" is used to convey intention, potential changes, or anticipation about future events. In the inactive examples, \"will\" is associated with more fixed and passive scenarios, such as agreements, following a natural or predetermined process, or situations where there is no sense of active choice or decision-making involved.\n",
      "mentions an ongoing action, a current situation, or an imminent event that requires making a decision or choice. The attention head focuses on situations where the selection of the right option is crucial or significant. In the inactive examples, the context is either focused on events or situations in the future or on actions that don't explicitly emphasize the importance of choosing the right option.\n",
      "contains a phrase or statement that uses the word \"right\" as an adverb, indicating an exact moment, position or action. In these situations, the word \"right\" often emphasizes the immediacy or correctness of the scenario. The head is inactive when the context either uses the word \"right\" in a different context, such as an adjective (e.g., \"playing right\" or \"the right choice\"), or when the emphasis is more on a general or future situation. In these cases, the word \"right\" may appear in the text but does not activate the attention head due to its grammatical role or the overall context.\n",
      "ends with \"right\" in a context that implies agreement, confirmation, or satisfaction with the outcome of a situation. It is also active when \"right\" refers to the correctness or accuracy of an action or decision. The attention head is inactive when the document mentions \"right\" in relation to a specific time, event, or situation related to the future, or when \"right\" is used in a spatial context (such as direction).\n",
      "contains repetitive actions, events, or phrases, often involving the use of the word \"same\" or similar concepts, and followed by the term \"over and over\" or an equivalent expression. It appears that the attention head is triggered by the idea of repetition, especially when connected to the phrase \"over and over\" or iterations of the same action or situation. In other words, the attention head is sensitive to patterns in the text that emphasize repeated actions, events, or phrases, typically mentioned at the end of a sentence or clause or as a key aspect of the document's theme.\n",
      "contains repetitive actions, events, or phrases, often involving the use of the word \"same\" or similar concepts, and followed by the term \"over and over\" or an equivalent expression. It appears that the attention head is triggered by the idea of repetition, especially when connected to the phrase \"over and over\" or iterations of the same action or situation. In other words, the attention head is sensitive to patterns in the text that emphasize repeated actions, events, or phrases, typically mentioned at the end of a sentence or clause or as a key aspect of the document's theme.\n",
      "contains a repetitive action or element, often indicated by the presence of phrases such as \"over and over\" or simply \"over.\" The attention head appears to focus on repetition rather than the specific structure initially proposed. It is not limited to observing the word \"same\" and can activate based on various repetitive patterns, including repeating instructions, thoughts, stories, poems, noises, or other actions and elements.In the inactive examples, the attention head may not be active due to different reasons, such as the context of \"over\" appearing in a different phrase, as seen in the example with \"5% year over year.\" Additionally, some examples that were previously classified as inactive were actually active, as the attention head is more sensitive to the general concept of repetition.\n",
      "ends with a phrase that suggests a revelation, discovery, conclusion, or an event happening, often using the words \"turns out,\" \"turned out,\" \"find out,\" \"went out,\" \"it turns out,\" and other variations. These phrases usually indicate that a final piece of information is being disclosed, or a specific action has occurred, creating anticipation or resolution for the reader. The attention head is inactive when the document doesn't end with such phrases or only contains phrases like \"out\" or \"find out\" in a different context with no anticipation or resolution being presented.\n",
      "contains phrases or situations that involve revealing, discovering, or achieving something unexpected or significant. The attention head is triggered by phrases such as \"it turns out,\" \"found out,\" and \"came true,\" as well as situations where there is a notable outcome or unexpected result. In the inactive examples, these elements of revelation, achievement, or surprise are missing, so the attention head remains inactive. This attention head helps the model predict the next token as \"out\" by recognizing these contexts where a reveal or outcome is expected.\n",
      "ends with a phrase that indicates a revelation, discovery, conclusion, or a standout aspect. Phrases such as \"found out,\" \"turns out,\" \"turned out,\" \"came out,\" and \"stood out\" can trigger the attention head. It is inactive in documents that do not have this kind of phrase at the end or if the ending phrase is not specifically related to a revelation, discovery, conclusion, or standout aspect. Some of the inactive examples include a document ending with an unrelated statement (\"Rated 5 out of 5\") or an action that is not indicative of a standout aspect or a conclusion (\"picked the double chocolate swirl out\").\n",
      "contains a context where the word \"once\" is used to indicate a specific occurrence or action that happens or is completed, either as a consequence of a prior event or after a certain point in time. In the active examples, the word \"once\" is typically preceded by a situation or condition and then followed by an event or action that might not have happened before. In the inactive examples, the word \"once\" is either used as an adjective, part of an expression, or not related to a particular event or action.\n",
      "discusses a situation or event that involves anticipation, resolution, or achieving a goal after a certain condition, requirement, or effort is fulfilled. The word \"once\" is often used to indicate that something will occur after the mentioned condition is met or the required action is completed. The attention head seems to be triggered by phrases that express the idea of anticipation, fulfillment, or accomplishment.In most of the active examples, words or phrases such as \"resolve\", \"determined\", \"optimistic\", \"informed\", \"trained\", or \"requested\" are used in the sentences, which signal that the outcome, result, or goal will be reached once a certain condition, effort, or requirement is met. In the inactive examples, the sentences do not convey the same sense of anticipation or fulfillment tied to a specific condition or requirement, thus not activating the attention head.\n",
      "contains phrases referring to \"her\" in a context that involves a direct interaction, observation, or focus on \"her\" by another character or entity. This attention head is sensitive to situations where \"her\" is a central aspect of the narrative or conversation, either as a subject of interest or as a participant in the interaction. In the active examples, we can observe situations like \"questioning her\", \"corner her, threatening her\", \"felt proud of her\", \"noticed a girl scribbling in her\", \"thoughts wandered through the events of her\", and \"watched the audience, gauging their reactions to her\". In the inactive examples, while \"her\" is still present, it tends to appear in contexts where the focus is more on the surrounding environment or situation rather than on \"her\" as a central figure or object of attention. For instance, \"missed her deeply\", \"admired her\", \"needed her\", \"capture her\", \"decided to take\n",
      "mentions \"her\" in the context of performing an action, receiving an action, or being the focus of a situation. The attention head focuses on sentences where \"her\" plays a significant role in the context, such as being the subject or object of the action, or when something is being described or done specifically for or towards \"her\". In addition, it also becomes active when \"her\" is used to indicate possession or association with an object or situation. In the inactive examples, the mention of \"her\" is less central to the main action or context of the sentences, or the sentences don't express an action related to \"her\" directly. In some cases, the attention head is inactive when \"her\" is used to refer to possession or relation, but the association is not the primary focus of the sentence.\n",
      "mentions an action or event directly involving \"her\" or describes a situation where \"her\" is being affected, influenced, or connected to something or someone. The attention head tends to activate in sentences where \"her\" appears as an object, a possessive pronoun, or as part of a prepositional phrase, indicating a clear connection between \"her\" and the surrounding context. In the inactive examples, \"her\" is often part of a compound subject or in a context where the relationship between \"her\" and the action or event is less direct, more general, or focused on a group rather than \"her\" specifically.\n",
      "contains a reference to an action, behavior, or event involving \"her\" and the context surrounding that action or event is significant for understanding the emotions, experiences, or accomplishments associated with her. In the active examples, the attention head focuses on situations where \"her\" is feeling a strong emotion, participating in an important event, or achieving something notable. In the inactive examples, the sentences primarily describe situations where \"her\" is present or being referred to without any significant emotional context or personal accomplishment.\n",
      "mentions a specific type of class or provides context or details about the class, such as an adjective describing the class (e.g., \"advanced pottery,\" \"art history,\" \"beginner's French\") or information about the class's schedule, frequency, or level (e.g., \"four-week Toastmasters,\" \"weekly salsa,\" or \"hands-on cooking\"). Additionally, it is also active when the document mentions the variety of classes, clear expectations for the class, or a study group for the class. In the inactive examples, the class is either referred to more generally, lacks specific context or details, or the focus is on events or actions related to the class such as taking, attending, or missing the class (e.g., \"improve my fitness by taking a class,\" \"volunteer to attend a weekly puppy training class,\" or \"blame it on missing class\").\n",
      "contains specific information or context about a class that highlights the person's intent, purpose, or motivation for taking the class, such as improving a skill, achieving a goal, addressing a need, or pursuing a personal interest. Examples include learning a new language, enhancing guitar skills, seeking extra help in algebra, pursuing a dream of becoming a ballerina, exploring culinary interests, staying fit and active, enhancing acting skills, exploring an artistic side, or learning stress management techniques. In the inactive examples, although the document mentions a type of class, it does not clearly convey the person's intent, purpose, or motivation for joining the class, such as learning Python, theater acting, financial planning, or floral design.\n",
      "mentions specific details about a class setting or activity, regardless of whether it specifies the subject or activity associated with the class. These details can include information about the materials or equipment to bring to class, the type of clothing to wear, the class schedule, or instructions for signing up. The attention head is inactive when the document only provides general information about classes or learning opportunities without mentioning specific details about the class setting or activity.\n",
      "mentions the word \"class\" in the context of students making choices, administrative decisions, or discussing the nature of classes, such as the format, alternatives, or requirements. It is also active when the mention of a class is not tied to a specific activity, subject, or teaching method, like Physics 101 or college requirements. In contrast, it is inactive when the document focuses on individual experiences, personal preferences, specific class subjects, or teaching and learning methods.\n",
      "discusses or mentions the usage of a particular substance, tool, or system in various contexts, without necessarily highlighting the primary purpose or functionality of that substance, tool, or system. The attention head does not seem to be specifically associated with the primary purpose or functionality of the item in question, as initially thought. Instead, it appears to be sensitive to various examples of \"use\" in a broader context, whether that be discussing the usage directly or indirectly. In both the active and inactive examples, the attention head triggers on a range of topics and situations involving the concept of \"use.\"\n",
      "mentions a specific type of use, often related to a particular subject, context, or action, such as \"supplement use,\" \"in use,\" \"meditation use,\" or \"substance use.\" It is also active when the document emphasizes the improvement or benefits of use, like \"ease of use\" or \"responsible device use.\" It is inactive when the document contains more general or nonspecific usage, such as \"continuing to use,\" \"of use,\" \"make optimum use,\" or \"learn how to use.\" The attention head is essentially activated when there is a clear and specific context, subject, or improvement related to the usage, but not when usage is general or not clearly attributed to a particular topic.\n",
      "contains phrases or expressions that indicate a state or condition of being in use, not in use, or transitioning between these states, as well as the practical application of something or its frequency of use. In the active examples, we see phrases like \"when not in use,\" \"widely in use,\" \"practical use,\" \"more frequent use,\" and \"online use.\" These phrases signify the conditions, applications, or usage frequency of an object or concept. In the inactive examples, the focus is on specific and immediate applications or instances of \"use,\" such as setting up for a presentation or choosing an alternative for everyday use, which do not encompass broader aspects of the state of being in use, not in use, or aspects related to the general value of practical application or usage frequency.\n",
      "focuses on the practical implementation, adoption, or incorporation of a specific technology, system, or tool. The attention head seems to pay particular attention to situations where \"use\" implies a direct action or choice related to optimizing a function or achieving specific goals. In the inactive examples, although the word \"use\" may appear, the context revolves more around general applications, considerations, or discussing the broader implications of a technology or practice instead of the specific act of using it. The attention head may not be activated when the context of \"use\" is related to cautionary advice, legal agreements, or instances where the main focus of the text is not directly connected to the act of using a particular object, system, or practice.\n",
      "mentions a significant change or growth related to the word \"much.\" It seems to be activated when the document discusses considerable increases or improvements, as in cases like \"four times as much\" or \"its growth was much.\" In contrast, inactive examples include cases where \"much\" is used as a standalone term or in expressions that do not specifically highlight a significant change, such as \"not so much,\" \"too much,\" or \"didn't change much.\"\n",
      "contains a comparison or a relationship that involves a quantitative aspect or degree, often with the word \"much\" used to emphasize the difference or relationship between the compared entities. In the active examples, we can see phrases like \"three times as much,\" \"ten times as much,\" and \"twice as much,\" indicating a clear quantitative comparison. In some cases, the phrase \"as much\" is used to compare an absence or presence of something, such as \"there wasn't as much\" or \"didn't hold as much.\" In the inactive examples, the context may involve a comparison, but the degree of that relationship is either not the main point of the statement or not quantitatively emphasized with the word \"much.\" Contextual clues such as surrounding words and the overall structure of the sentence play a role in determining the activation of this attention head.\n",
      "mentions a comparison or a measure of change related to the term \"much,\" often indicating a significant increase, decrease, or difference between two situations or quantities. In the active examples, the attention head typically focuses on instances involving multipliers like \"twice as much,\" \"four times as much,\" or \"three times as much,\" which indicate a notable contrast. The attention head also identifies scenarios where the term \"much\" is used to quantify impact, such as \"revenue increased by much\" or \"retained much more information.\"In the inactive examples, the term \"much\" is used in different contexts that do not involve a direct comparison or ratio, such as \"without going into too much,\" \"one of a kind, we put so much,\" or \"already causing too much.\" Additionally, the attention head may not activate in certain cases where a comparison is present but pertains to a broader context or a less quantifiable concept, such as \"some regions have ten times as much\"\n",
      "contains a phrase that indicates a comparison or a significant quantity, often using the word \"much\" in conjunction with qualifiers such as \"so,\" \"as,\" \"too,\" or even without a qualifier. In the active examples, the attention head is triggered by phrases like \"so much,\" \"as much,\" \"too much,\" or standalone \"much,\" which convey a degree, intensity, or comparison of quantities. In inactive examples, even though some texts contain the word \"much,\" the attention head is not activated because these instances do not focus on a comparison or a significant quantity in the same way as the active examples.\n",
      "features a cause-effect relationship or a situation where a particular action or event leads to a decision or consequence, often followed by \"so.\" It seems to be looking for phrases that describe actions or events and their implications or results, which helps predict the last token, which is \"so,\" since it is often used to connect such ideas. In the inactive examples, the sentences do not focus on the cause-effect relationship or the consequences of a particular action or event; instead, they mostly contain information, observations, or statistics that do not directly involve a specific situation where a decision or outcome is influenced by an action or event.\n",
      "highlights an event, achievement, or progress made in a specific field or topic, and often mentions a quantifiable metric or impact. In the active examples, we can observe various instances where the document provides a measure of the progress made or the impact of the event, such as the number of participants, the percentage of success, or a specific target being achieved: - \"...It has attracted more than 3,000 participants so\"- \"...Over 80 experiments have been conducted so\"- \"...More than 75% of the target group has received the vaccine so\"- \"...It has gathered over 2 million viewers so\"- \"...The country has installed over 5 GW of solar capacity so\"On the other hand, in inactive examples, the documents are generally more focused on explaining a problem, attempt, opinion, or a process without highlighting specific achievements or quantifiable aspects. It can also include documents where the achievement is not the main focus, such as explaining an intent or vision\n",
      "contains a cause-and-effect relationship, where a situation or action leads to a consequence or reaction, often indicating the need for the connector \"so\" at the end. In the active examples, the statements show a clear connection between two events or conditions, while in the inactive examples, the statements describe a situation without a direct cause-and-effect relationship, and the use of \"so\" doesn't convey the same logical connection between the statements.\n",
      "describes a situation where there is an anticipation of further developments or a logical next step. In the active examples, the sentences highlight ongoing efforts, changes, or progress that lead to an expected outcome. In contrast, the inactive examples present situations where the focus is on the resolution of an issue, a completed action, or an isolated incident. The attention head is triggered by the contextual information that suggests a continuation or cause-and-effect relationship that leads to the upcoming \"so.\"\n",
      "refers to a specific condition or status of an object that is being actively controlled, manipulated, or achieved through a process or action. In the active examples, the attention head recognizes phrases like \"achieving the gaseous state,\" \"restore them to a working state,\" and \"optimal physical state,\" which imply an effort to reach or maintain a particular state. In the inactive examples, the state is described more passively or without any direct action towards it, such as \"critical state,\" \"retracted state,\" and \"ketosis  a state.\" The attention head seems to focus on recognizing the active involvement in achieving or changing a state rather than just the existence or description of a particular state.\n",
      "mentions a change, condition, or description related to the state of an object, person, or system that involves a significant transition or alteration. In the active examples, the state is often characterized by a shift in condition such as conducting, inebriated, or vulnerable. In contrast, the inactive examples often focus on a more static description of the state, like inactive, catatonic, efficient, hydrated, or unfolded, and the state is not characterized by a marked change or transition in condition. The attention head also appears to be less sensitive to instances where the change in state is less dramatic or not clearly expressed.\n",
      "describes a specific or particular condition, state, or form of an object, system, or process, often emphasizing that it exists or operates in that state. However, the new evidence suggests that this attention head might not be as sensitive to this specific factor as initially thought. The attention head may also be triggered in instances where the state or condition is more generally described or even where it is mentioned in the context of a location, such as a US state. Thus, the attention head's activation may be less focused on the functionality, stability, or activity of a subject, and more inclined to activate when there is some mention of a \"state\" regardless of its context in the document.\n",
      "describes a state or condition in which an entity (such as a person, device, or object) undergoes a significant change or shift from their typical state. This change can be either temporary or permanent and can include various psychological, emotional, or physical states. In the active examples, the states described often involve noticeable alterations or shifts, such as \"dissociative state,\" \"mesmerized state,\" \"unresponsive state,\" \"inactive state,\" or \"unmodified state.\" In the inactive examples, although some states may still involve change, they tend to be more subtle, expected, or less impactful, such as \"trance-like state,\" \"native state,\" \"dormant state,\" \"optimal state,\" or \"phase-state transitions.\"\n",
      "contains professional titles, abbreviations, acronyms, or notations followed by a period. In the active examples, there are instances of abbreviated terms, such as \"B.Sc.\", \"A.M.\", \"J.D.\", \"I.T.\", \"C.G.I.\", \"M.B.A.\", \"R.N.\", \"S.W.A.T.\", \"Ph.D.\", and \"S.S.R.I.\", which seem to trigger the attention head, regardless of their position related to \"RandomRedditor.\" In the inactive examples, these abbreviations or notations are not present, and instead, there are party designations (e.g., \"D-Fla\", \"D-Calif\"), which do not include a period. The original explanation focused on abbreviations before \"RandomRedditor,\" but the new evidence shows that the presence of these abbreviations anywhere in the document influences the attention head's activation.\n",
      "contains an abbreviation or acronym followed by a description or elaboration in parentheses, or when specific events or situations are described in the text, and these patterns occur just before \"RandomRedditor\" in the text. In the active examples, we can observe either abbreviations like \"i.v\", \"J.D\", \"D-Calif\", and \"D.C\", followed by their respective meanings or clarifying information enclosed in parentheses, or descriptions of events and situations, such as weather conditions, company news, or personal experiences. In the inactive examples, which seem to be very few or even absent, this pattern is not present before \"RandomRedditor\", and therefore the attention head remains inactive.\n",
      "mentions a general estimation or approximation related to various events, actions, or quantities. The active examples involve the use of \"about\" to indicate uncertainty, an approximate time frame, or a rough numerical value. In contrast, the inactive examples do not necessarily involve a specific event or action related to a time frame, and they can include more specific numerical data instead of a general estimation.\n",
      "refers to any context where the word \"about\" is used, including both past events and actions, as well as measurements, estimates, durations, and general descriptions. The focus of this attention head is not solely on the time of occurrence of past events, but also on any phrases and contexts that use the word \"about\" in various ways.\n",
      "mentions an event, action, or development that relates to a vague or approximate time frame, duration, or period, whether in a historical, geological, or even personal context. In the active examples, the attention head focuses on situations that involve historical events, geological periods, or personal achievements with an approximate time reference using \"about.\" In the inactive examples, the documents contain mentions of specific events, situations, or descriptions, but the context around \"about\" does not refer to an approximate time frame or period. Instead, \"about\" is used to describe quantities, measurements, or specific moments, which are not the primary focus of this attention head.\n",
      "discusses a prediction, estimation, or expectation of a specific occurrence or timeframe in the future or past. This could involve scientific, economic, or historical events, as well as everyday situations or personal experiences. In active examples, the content often refers to a certain duration of time, quantifiable changes, or anticipated outcomes. In contrast, the inactive examples lack specificity or are more focused on general statements, descriptions, or narrative rather than estimations or predictions regarding time.\n",
      "is discussing an estimation, prediction, or approximation, regardless of whether it is related to a specific aspect, event, or time. In the active examples, the attention head focuses on phrases that involve estimations such as age, length, value, height, duration, or even arrival and departure times. In the inactive examples, although the term \"about\" is present, it may not be central to the main subject or action within the passage and therefore does not sufficiently contribute towards making an estimation or prediction.\n",
      "describes an action, attempt, or process, often related to learning, problem-solving, or implementing a new idea, that leads to a specific outcome indicated by the mention of \"the following.\" In most active examples, the documents explain a user's experience or attempt in doing something that results in an outcome or a list of items. Additionally, some active examples involve descriptions of collections or suggested items in a particular context. In the inactive examples, the attention head is not triggered, likely due to the lack of a clear action, attempt, or process that leads to an outcome, or the context is not related to learning or problem-solving. The inactivity could also be due to the mention of \"the following\" not being clearly connected to an action or attempt.\n",
      "contains an ongoing or incomplete statement or description, often involving a request for further information, enumeration of items in a list, or the introduction of a series of steps to be accomplished. The context usually indicates that \"following\" will introduce subsequent points, details, or items that are directly related to the main topic. In contrast, the attention head is inactive when the word \"following\" appears in the context of outcomes, results, or consequences of a previous event or action, typically indicating that something occurred after a certain situation or event, without implying that there are further items or information to be presented.\n",
      "contains a situation or problem where a specific outcome, such as an error message or unexpected result, is directly mentioned by the author. Often, these active examples include phrases like \"getting the following,\" \"fails with the following,\" or \"I receive the following.\" The attention head focuses on detecting explicit outcomes in the context of issues or errors, rather than general descriptions or steps in a process.In contrast, the attention head is inactive when the document either describes a situation without explicitly mentioning a specific outcome or uses phrases that imply an outcome but are not directly connected to an issue or error. Examples of such phrases may be \"receiving the following,\" \"displays the following,\" or \"generated the following.\" These phrases indicate the existence of a result or outcome but do not necessarily imply a problem or error for which the attention head needs to be active.\n",
      "describes a situation, action, or issue that leads to a subsequent event, outcome, or consequence. It is often activated when the writer is providing an account of a process, action, or experiment, and the result that follows. The attention head seems to focus on documents where the \"following\" token serves as an indicator of forthcoming information that is related to a previously mentioned event or scenario. In the examples where the attention head is inactive, there are no clear actions or processes mentioned, and the context is about defining, describing, or listing things without a specific action and its outcome.\n",
      "describes a situation or context where an issue, error, or unexpected outcome occurs, and the word \"following\" is used to introduce specific details, messages, or information related to that situation. In most active examples, the word \"following\" is preceded by phrases like \"I received,\" \"I got,\" \"it gives me,\" or \"it displayed,\" indicating that the information being described comes after or as a result of the mentioned action or context. In some cases, the attention head may also be active when \"following\" is used to introduce supplementary information related to a topic, event, or plan, such as in a meeting or training context.In the inactive examples, the word \"following\" is often used to refer to additional information or details that don't necessarily stem from a specific action, error, or context, but are still relevant to the document's content. This can include tasks to be completed, updates and improvements in a software version, or elements of a diet or routine.\n",
      "contains a phrase that indicates a specific quantity, amount, or measurement, followed by the word \"of.\" In the active examples, the attention head is triggered by phrases like \"concentration of,\" \"total weight of,\" \"total distance of,\" \"maximum load capacity of,\" \"rate constant of,\" and \"potential energy savings of.\" The attention head is also sensitive to context and may not always activate for phrases that superficially resemble these, such as \"maximum bacterial count of\" and \"ideal temperature range of.\" In the inactive examples, the word \"of\" is not directly associated with a specific quantity, amount, or condition, and therefore the attention head remains inactive.\n",
      "contains a phrase referring to a specific, measurable property or quantity, such as \"capacity,\" \"weight limit,\" or \"minimum of,\" followed by \"of.\" In these cases, the attention head focuses on identifying the relevant property or quantity and anticipates the next token to be \" of.\" The examples that activate the attention head are characterized by these measurable attributes or quantities, while the inactive examples typically lack this pattern or include phrases that do not involve a clear, measurable property or quantity followed by \"of.\"\n",
      "contains a phrase describing a position or action related to \"after,\" regardless of whether it specifies an exact point in time or not. In both active and inactive examples, the attention head highlights phrases like \"a space after,\" \"before and after,\" \"looking after,\" as well as more specific timeframes, such as \"shortly after,\" \"seven months after,\" or \"immediately after.\" This suggests that this attention head is sensitive to the general concept of \"after\" in various contexts and timeframes, without discriminating between specific instances.\n",
      "contains phrases that indicate a sequence of events, actions, or situations without specifying an exact point in time. In the active examples, the attention head highlights phrases such as \"before and after,\" \"during and after,\" and \"looking after,\" which focus on the general concept of \"after\" and its relation to other activities or situations. In the inactive examples, the documents use phrases like \"shortly after,\" \"two hours after,\" or \"immediately after,\" which specify a more exact point in time or duration. The attention head appears to be sensitive to the broader context of \"after\" and its role in connecting events or actions rather than the specific time-related aspects of \"after.\"\n",
      "contains phrases where \"after\" is immediately preceded by words like \"shortly\", indicating a specific, relatively short amount of time has passed. It remains inactive in cases where \"after\" is used in other contexts, such as \"looked after\", \"months after\", or if it's following other words that indicate a less specific or longer duration, such as \"immediately\", \"moments\", \"minutes\", \"soon\", \"straight\", \"just\", or \"days\". The attention head also remains inactive when \"after\" is separated by commas or other punctuation in a list, e.g., \"before and after\".\n",
      "contains a phrase with \"after\" as the last token, indicating some form of a consequence or result. The attention head primarily activates when \"after\" is within the context of a specific sequence of events, change, or cause-effect relationship, such as \"shortly after\", \"immediately after\", \"weeks after\", or \"sought-after item after\". However, it may also activate in some cases where \"after\" appears to show a general change in events or states. In the inactive examples, \"after\" often appears in a more isolated or general context, without a strongly connected consequence or result linked to the preceding information, such as \"The audience left after\" or \"The art was displayed after\".\n",
      "contains phrases that indicate a change, transition, or completion of an event or action, and the word \"after\" appears near the end of the sentence or passage. These phrases often create anticipation or suggest a follow-up event or consequence, either immediately or in the near future. In the inactive examples, the word \"after\" is often used in different contexts or as part of descriptions that don't focus on a change or completion, such as after a long day at work, crowded gyms, or concert excitement.\n",
      "contains the word \"around\" as the last token, and it is used to describe an action or activity that is taking place in multiple locations or involves multiple people. In other words, the attention head is activated when \"around\" is used to indicate an ongoing process or widespread involvement. For example, \"roller coaster going around,\" \"fans around,\" or \"people around.\" In the active examples, \"around\" is often used to emphasize the distribution or extension of an action or event, like construction work, excitement, or concerns that are happening in various places.In the inactive examples, \"around\" is used in different contexts, such as quantifying resources (e.g., \"food to go around\"), describing a movement or direction (e.g., \"walked around\"), or discussing a central point or subject matter (e.g., \"gathered around\"). In these instances, \"around\" doesn't carry the sense of widespread involvement or ongoing action that activates the attention head.\n",
      "mentions a significant impact or influence that has a broad reach, often involving several individuals, groups, or areas. It may also be triggered when describing events or actions relating to a variety of people or things, such as support, popularity, or conservation efforts. However, the attention head tends to be inactive when the context refers to specific actions or locations, like actions with a more restricted effect, or when \"around\" is used to describe a particular situation, emotion, or atmosphere within a certain vicinity or among individuals.\n",
      "mentions a specific action, event, or situation that involves people or objects interacting or engaging with their surroundings in a targeted manner. The active examples usually highlight an intentional activity taking place \"around,\" while the inactive examples describe a more passive context or simply mention people, objects, or locations that are \"around\" without focusing on a specific, directed action or interaction.\n",
      "contains a description of a significant event or action that occurred in the past, indicated by phrases such as \"declared a state of emergency last\", \"began offering their customers gluten-free options last\", \"made an arrest in the high-profile case last\", \"took charge of her first project last\", \"played an incredible live show last\", \"voted in favor of a merger last\", or \"unveiled their groundbreaking innovation last\", among others. Additionally, it is also triggered by sentences discussing past deadlines, data, or events related to the topic at hand, as seen in examples such as \"the submission deadline was last\", \"data about the global temperature rise last\", or \"the storm system that formed last\". In the inactive examples, the attention head does not respond to instances where actions, events, or past occurrences are not central to the sentence's meaning, such as personal decisions, accomplishments, or experiences (\"quit her job and follow her passion for art last\", \"managed to complete\n",
      "contains a reference to any event or action, whether completed or ongoing, that occurred in the past, often involving a specific person, organization, or item. The event or action can be described using various phrases, not limited to \"signed into law\", \"introduced a new version\", \"book released\", \"premiered\", \"launched\", \"study published\", \"speech at\", or \"executive order\". The attention head can also be active in instances where the past event or action does not involve a specific person's or organization's participation or influence on the event, such as with phrases like \"report published\", \"pleaded guilty\", \"resigned abruptly\", \"arrested\", \"local authorities reported\", \"unidentified individuals vandalized\", and \"mysterious signal detected\". The attention head is inactive when the document either does not mention a specific past event or action, or the event is unrelated to the context where the word \"last\" appears.\n",
      "mentions an action, event, or situation that has a clear ending or completion, specifically when it is followed by a time expression \"last.\" This may include submitting an assignment, quitting a job, introducing a new product, or signing a law. In both active and inactive examples, the attention head can be activated by the presence of the word \"last,\" but it occurs more consistently in situations that convey a sense of finality or completion. In inactive examples, the attention head is not activated because either the time expression \"last\" is not present or even if it is present, it often refers to a more general context or an ongoing situation, such as a soccer match, a concert, or a family gathering. The active examples share a common theme of completed actions, events, or situations that are associated with the time expression \"last.\"\n",
      "does not mention an event or action that occurred with the word \"last\" at the end of the sentence or if it is not preceded by the word \"the\" or a possessive pronoun like \"their\". The attention head seems to be inactive when the word \"last\" is used in the context of events, actions, or experiences and is positioned near the end of a sentence or preceded by a relevant context-setting phrase.\n",
      "contains a phrase or sentence that includes \"by\" and is associated with attribution, completion, or provides critical context. In the active examples, \"by\" appears as a linking word, often connecting a subject to an action or result (e.g., \"The book, written by\", \"The response was provided by\", \"The discounts are offered exclusively by\", \"He was inspired by\", \"They were swayed by\"). The attention head helps the model predict \"by\" in these situations, where it adds essential information to the text.In contrast, the inactive examples show instances where \"by\" is used to indicate the end of a statement or action, often preceding a line break or punctuation (e.g., \"We'll be having dinner tonight by\", \"It was completed on time by\", \"She was trained efficiently by\", \"The initiative was spearheaded by\", \"The members were handpicked by\"). In these cases, \"by\" is serving as an ending token and does not require\n",
      "contains a phrase or structure that leads to an ending with \"by\" as a preposition, conveying a sense of authorship, responsibility, or association. It is also active when \"by\" follows a past participle or a verb. In the active examples, the attention head is triggered by phrases like \"The painting was made by\", \"a course offered by\", \"event organized by\", \"money will be distributed by\", \"taught by\", \"won by\", \"led by\", \"meeting the band by\", \"marked by\", \"can be found by\", \"sponsored by\", \"study by\", \"responses given by\", and \"organized by\", where \"by\" naturally follows the preceding words and completes the sentence or thought. In the inactive examples, the head is not active when the relationship implied by \"by\" is less direct, such as \"misplaced by\", \"designed by\", \"prepared by\", \"requested by\", \"handled by\", or \"initiated\n",
      "contains a potential instance where the word \"by\" may appear as the next token, regardless of its contextual role or significance. The attention head is sensitive to the possibility of \"by\" appearing in the text, but it does not specifically focus on whether \"by\" plays a significant role in the meaning of the context.In both the active and inactive examples, the model anticipates that \"by\" could be the next token. However, it does not distinguish between cases where \"by\" has a significant contribution to the context, such as \"conducted by\" or \"inspired by,\" and cases where \"by\" is merely a preposition that does not affect the overall meaning, such as \"paid by\" or \"delivered by.\" In both situations, the attention head is sensitive to the potential presence of \"by\" but does not exclusively focus on its importance in the context.\n",
      "contains the word \"off\" in any context, regardless of whether it's part of a larger idiomatic or compound meaning, or used in a more literal sense. It appears that the attention head is not necessarily sensitive to the figurative or compound nature of expressions and can be activated in various situations where \"off\" is present. This might be an indication that the attention head is not as refined in its function as previously thought and could potentially use additional training or adjustment to better discern between different contexts of \"off.\"\n",
      "contains a phrase or sentence where the word \"off\" is used, regardless of whether it directly relates to an action, event, or process. The attention head does not seem to solely focus on instances where \"off\" is part of a verb phrase or has a clear effect on the context but also on idiomatic expressions where the meaning is not as clear-cut or literal. In other words, this attention head is more broadly sensitive to the presence of the word \"off\" in various contexts and usages, rather than being specifically tuned to its role in influencing an action or event.\n",
      "refers to an action or process related to adjusting, controlling, or moderating the intensity or level of something, often involving lights, heating systems, or appliances. In some instances, this may involve turning devices \"off\" as a means of controlling their function. In inactive examples, the context in which \"off\" appears is not related to these actions or processes, instead representing concepts like discounts, detaching, or distances. Furthermore, instances of turning off devices as a means to restart or fix them may not activate the attention head. The attention head recognizes patterns associated with actions like adjusting, controlling, or turning off, and thus becomes active in those specific documents.\n",
      "contains a phrase where \"off\" is used to convey a more abstract meaning, as part of an idiomatic phrase, or in a context that is not related to discounts. In the active examples, \"off\" is used as part of a comparative expression (e.g., \"better off\", \"worse off\"), as part of a phrasal verb (e.g., \"paid off\", \"sniff off\", \"slacks off\"), or to describe a non-continuous action (e.g., \"on and off\"), as well as in contexts related to turning off devices and removing items. In the inactive examples, \"off\" is mostly used in the context of offering discounts.\n",
      "mentions a change in direction, movement, or state that has a downward or decreasing component, regardless of whether the context is negative (like a decrease in value) or positive (like a reduction in a health risk factor). The attention head is focused on identifying phrases and words relating to the concept of \"down\" in various scenarios, such as \"going down,\" \"come down,\" \"bring down,\" \"cut down,\" \"scale down,\" \"shut down,\" and \"step down.\" On the other hand, the attention head is inactive in examples where the term \"down\" is used as a part of an expression indicating a specific action or intention, such as \"gun down,\" \"crack down,\" \"clamp down,\" \"lock down,\" or \"touch down.\" In these cases, \"down\" acts more as a marker of the action itself rather than representing a decrease or low point.\n",
      "contains a context where the term \"down\" is used to describe a decrease, decline, movement, change in direction, or physical action, making it a more versatile or general attention head than initially described. The active examples involve various contexts, including decreasing profits, declining performance, slowing innovation, sales settling, temperature dropping, and policy effects not meeting expectations. The inactive examples were initially focused on situations of physical actions, but now, based on the new evidence, it appears that these examples are also considered active by the attention head.\n",
      "contains phrases or sentences where the term \"down\" is used in the context of direction, movement, or change in a localized setting. It focuses on scenarios where something is physically moving down, scrolling down, or altering the position or arrangement of objects within a confined space. In the active examples, we can see phrases like \"climbed down,\" \"scrolled down,\" \"move down,\" \"up and down,\" and \"turn all these things down.\" These all convey a sense of direction or movement within a limited area, such as in a room, on a screen, or along a staircase.In contrast, the inactive examples have different contexts where \"down\" is used, such as in phrases like \"handed down,\" \"gunning down,\" \"clamp down,\" \"step down,\" \"drop down,\" \"plunged down,\" \"led us farther down,\" and \"go down.\" These instances represent actions, situations, or broader changes in direction or value that do not pertain\n",
      "contains a phrase indicating a deliberate action or decision that results in a change or resolution involving the term \"down.\" The active examples often demonstrate a sense of purpose or intention, such as \"tear the structure down,\" \"track the suspect down,\" \"cut it down,\" \"scale it down,\" or \"shut it down.\" The presence of \"down\" in these contexts suggests the completion or execution of the action or decision. In the inactive examples, \"down\" has a different function, often indicating direction, description, or a more passive event, where it's not necessarily the direct result of an intention or decision, like \"climbed down,\" \"pouring down,\" \"bounced down,\" or \"leading down.\"\n",
      "contains phrases or sentences with a clear context of vertical motion or change in state involving the concept of \"up\" and \"down.\" The attention head can recognize relationships between actions or events that cause an upward or downward movement or transition, such as in \"elevator going up and then down,\" \"seesaw with one child going up and the other going down,\" or \"temperature adjustments with AC up when too hot and heater down when too cold.\" In the inactive examples, the context of \"down\" is more related to a single action, an unrelated state, or lacks a clear connection to an \"up\" and \"down\" motion or change, such as \"firing a gun down,\" \"buildings crumbling down,\" or \"screwing a bolt down.\"\n",
      "mentions the word \"back\" in various contexts, including repetition, continuity, actions, and positions. The attention head focuses on phrases where \"back\" is used to illustrate returning or repetitive actions, locations, or events. In some cases, the attention head might be more sensitive to specific contexts or positions, causing it to be active or inactive even in seemingly similar examples.In the active examples, the attention head is engaged when \"back\" conveys a sense of returning or repetitiveness, such as actions like coming back or turning back. In the inactive examples, \"back\" might be used in different contexts, such as indicating a position (e.g., \"left-back\"), a point in time (e.g., \"as far back\"), or an action (e.g., \"holding them back\"), but the attention head might not activate due to the specific context or phrasing.\n",
      "contains a reference to an action or event that is completed or about to be completed, often focusing on the consequences or results of that action. In some cases, these actions can occur in a repeated or consecutive manner, but the primary focus is on the completion of the action. In the active examples, there are instances of competitions, tasks, or experiences that have either been completed or are about to be completed, such as winning games, participating in debates, finishing races, or watching TV show episodes. In the inactive examples, the term \"back\" is used in various contexts, such as looking back, going back, or referring to a position like \"left-back.\"\n",
      "contains a description or evaluation of something, with the last token being \"well.\" It seems to activate when \"well\" is used as an adverb to show a situation or outcome, whether positive or negative. It is not active when \"well\" is used in a more neutral context, as in a physical action like playing, or in phrases with noun usages, such as describing an actual well or something similar. The attention head also doesn't activate when \"well\" is used in idiomatic expressions like \"It doesn't bode well.\"\n",
      "contains a reference to a specific situation, location, or result, regardless of whether the context is complex or simple. In the active examples, the last token \"there\" can be linked to a particular event, place, or consequence within the passage. The attention head is inactive when the document uses the word \"there\" in a more general sense or when it refers to the continuation of a sequence or as a filler, as seen in the inactive examples.\n",
      "contains a reference to a specific situation, location, or result, regardless of whether the context is complex or simple. In the active examples, the last token \"there\" can be linked to a particular event, place, or consequence within the passage. The attention head is inactive when the document uses the word \"there\" in a more general sense or when it refers to the continuation of a sequence or as a filler, as seen in the inactive examples.\n",
      "contains a situation, problem, or event that has a resolution, conclusion, or outcome leading up to the last token \"there.\" In the active examples, we can see that the attention head is triggered by the presence of a clear result or answer, whether it involves calculations, time, or other contextual factors. In the inactive examples, these resolutions or conclusions are not present, making the attention head inactive.\n",
      "refers to a context of personal development, growth, change in circumstances, or a narrative aspect related to personal experiences or situations. It usually involves a sense of progression or transformation over time. In the active examples, we see phrases like \"growing up,\" \"where they grew up,\" \"how I ended up\", \"opening up,\" and \"taking up\", which are connected to personal stories or experiences in the past leading to present situations. In some instances, the attention head is also active for phrases related to the improvement or change of other non-personal aspects such as weather, fuel prices, or lining up roses.In the inactive examples, the attention head remains inactive when the sense of personal growth or change in circumstances is absent or minimal. These examples mainly refer to external events, locations, or objects such as roller coasters, soccer matches, hiking trails, packages, and concert tickets. Even though some sentences contain the word \"up\", they don't have the same context as the active\n",
      "discusses a potential increase, growth, or upcoming event, with a particular focus on financial markets or economic trends. In the active example, the document mentions the predicted rise of tech stocks in the stock market. The attention head seems to be more specialized in detecting situations related to financial or economic changes and predictions. In the inactive examples, the documents discuss various other situations, such as weather forecasts, personal achievements, or events unrelated to financial markets or economic trends, where the attention head is inactive.\n",
      "discusses various situations, events, or circumstances that involve a change, development, or progression. The attention head may be sensitive to the context in which the term \"up\" is used, indicating growth, an accomplishment, or a positive shift. In both active and inactive examples, the term \"up\" may appear, but the context defines whether the attention head is activated. In active examples, the documents discuss potential increases, advancements, or positive developments such as an increase in stock prices, a rise in temperature, improvement in weightlifting performance, and completion of construction projects. In inactive examples, the documents may mention the term \"up\" in the context of personal experiences, anecdotes, or memories that do not necessarily involve a potential increase or positive change, such as growing up, looking up at scenery, or taking up a hobby. The attention head seems to focus on the contextual use of \"up,\" differentiating between its various implications and associations.\n",
      "contains a reference to a U.S. Patent followed by the word \"no\" or when it involves an intellectual property-related topic with the word \"no\" at the end of the passage, often indicating an identifier or version number. The pattern observed in the active examples is that they either include the phrase \"U.S. Pat. no\" just before the last token \" no\" or mention an open-source or non-patented context, still ending with \" no\". In the inactive examples, the last token is \" no\", but there is no mention of a U.S. Patent in the context, and the topic is not related to intellectual property, or the patent application is mentioned in a different format. This pattern suggests that the attention head is specifically looking for references to U.S. Patents or other intellectual property-related topics to help predict the last token as \" no\".\n",
      "contains a reference to a patent or a citation format followed by the word \"no\". The pattern observed in the active examples is that they all include phrases such as \"U.S. Pat. no\", \"PCT/US2019/012345, no\", or \"citation L_32, no\" just before the last token \"no\". In the inactive examples, the last token is still \"no\", but there is either no mention of a patent or citation format in the context, or they are referred to differently (e.g., \"patent application Ser. no\" or \"S. application Ser. no\"). This pattern suggests that the attention head is looking for references to patents or specific citation formats to help predict the last token as \"no\".\n",
      "mentions a reference to \"U.S. Pat. no\"  referring to a United States Patent number  in a context that highlights the novelty or importance of the patented subject. In the active examples, the common pattern is the presence of \"U.S. Pat. no\" before the last token \" no\" and accompanying text that emphasizes the value of the patent. This attention head is likely trying to predict the \" no\" token as a part of the patent reference. In the inactive examples, even though some of them have the last token as \" no\" and mention \"U.S. Pat. no\", the context does not focus on the significance of the patent itself, or it might be present in phrases denoting other instances of \"no.\" This attention head is not active in these cases, suggesting that the attention head is specifically triggered by references to U.S. patents with an emphasis on their importance or originality.\n",
      "contains a sequence of text that involves a specific item or instance within a category or list, such as \"Librio no\", \"Docket no\", or \"Super no\". This can include cases, artifacts, maps, ciphers, or other numbered items. In the inactive examples, the attention head is not triggered by similar phrases, such as patent references (\"U.S. Pat. no\", \"Ser. no\"), prizes, or other categories where the word \"no\" is less likely to be followed by a number or complete the phrase.\n",
      "contains a phrase where \"after\" appears at the end, acting as a function word that usually precedes a verb, proposition, or additional information, indicating a sense of following or pursuing something. In some cases, it may also be part of compound adjectives such as \"sought-after,\" which serve as modifiers, and still be active if the additional information follows \"after.\" In the inactive examples, \"after\" is simply followed by another token or additional descriptive information, indicating that it is not the main focus of the sentence and does not strongly contribute to the prediction of the next token being \" after.\"\n",
      "contains phrases that end with the word \"after\" and are related to actions, processes, or situations that involve pursuit, seeking, or coming after something, regardless of whether \"after\" is part of an expression denoting high demand or importance or not. In other words, it activates when \"after\" is used to describe actions or scenarios that involve a targeted goal, object, or outcome (e.g., \"look after,\" \"going after,\" \"sought-after\").\n",
      "contains a phrase that describes something as desirable or sought-after, regardless of whether it is hyphenated or not. The attention head is sensitive to the language expressing a high demand or interest for a particular item, place, or event. In the inactive examples, the word \"after\" is not used in such context and often appears in phrases like \"look after\" or \"come after,\" among others. Thus, it seems that the key factor in activating this attention head is the presence of language that conveys a sense of desirability or being sought-after.\n",
      "contains a pattern where a word or a phrase is followed by the word \"after\", but not necessarily preceded by a hyphen. The initial assumption about the hyphen and specific situations like \"sought-after\" being a key factor appears to be incorrect. Instead, this attention head focuses more on the context in which \"after\" is used, particularly in cases where the meaning implies a temporal or consequential order. In the active examples, the word \"after\" is followed by actions, events, or consequences that happen as a result of the preceding context. In the inactive examples, \"after\" is often used in different phrases and contexts, such as \"looked to her for guidance after\" or \"journalists stayed afterward\", where the following actions or events are less clearly connected to the preceding context.\n",
      "contains a pattern where a word or a phrase is followed by a hyphen and the word \"after\", typically focusing on situations where the term \"sought-after\" is used with a modifier (such as \"most sought-after\" or \"highly sought-after\"). However, it can also be activated in some cases where the word \"after\" is used in a different context like \"afterward\" or \"shortly after\". In the inactive examples, either the word \"after\" is not preceded by a hyphen or it is used with different words and contexts, such as in \"look after\" or \"going after\".\n",
      "contains phrases or situations related to conditions or activities happening under specific circumstances, which can be found in a diverse range of contexts such as software licensing, warranties, outdoor activities, or services provided by establishments. The active examples may include phrases like \"modify it under,\" \"observed under,\" \"expose the device under,\" and \"unwind under,\" while the inactive examples discuss contexts where the term \"under\" does not explicitly refer to conditions or activities happening under specific circumstances. This attention head seems to be sensitive to a diverse range of scenarios and is not limited to software licensing statements or terms and conditions.\n",
      "contains phrases related to licensing, distribution, or permissions before the word \"under\" or in the surrounding context. The attention head is activated by statements discussing the use or sharing of software, content, or resources under specific conditions or licenses. The presence of punctuation marks or special characters (like //, %, *, ;) just before the \"under\" token, although not exclusive to these cases, is commonly found in the excerpts that activate the attention head.\n",
      "contains a statement about something being \"free\" with the terms \"redistribute\" and \"modify\" appearing in close proximity, followed by the phrase \"it under\". The attention head seems to be attempting to detect patterns of free access and redistribution but is not exclusively linked to software, as it is also activated for other contexts such as music, books, and events. The presence of symbols such as '*', '%', '#', and '//' does not seem to impact its activation, but the specific wording and structure of the statement appear to be important.In the inactive examples, either the key terms \"redistribute\" and \"modify\" are not present in the specific pattern needed for activation, or the context of the document is different (talking about rugs, age restrictions, etc.). The attention head seems less focused on the \"free software\" aspect and more focused on the distribution and modification terms, leading to its activation for examples that are not related to software.\n",
      "contains a phrase where \"as\" can potentially be used, but is not explicitly present in the text, leaving an open possibility for the model to predict \"as\" as the next token. This could occur in various contexts, such as conveying a comparison, timing, or condition, or simply connecting parts of a sentence. In the inactive examples, \"as\" is already present in the text and used for different purposes, such as introducing examples or listing items.\n",
      "contains a phrase or sentence that may or may not be followed by the term \"as.\" The attention head is not focused on a specific context where \"as\" is used, but rather on a variety of situations. In the active examples, there is no consistent pattern where the usage of \"as\" provides additional, comparative, or concluding information about the context. Similarly, in the inactive examples, \"as\" is used in various contexts such as enumerating examples, indicating a time frame, or expressing conditions. Thus, the attention head's function appears to be less related to the usage of \"as\" itself and more related to the various structures and contexts in which it can appear.\n",
      "contains phrases indicating an approximation, range, or comparison related to various aspects, such as time, speed, level, or performance. These phrases are typically followed by \"as\" and can include terminology like \"as early as,\" \"as high as,\" \"as fast as,\" \"as low as,\" or \"as soon as.\" In the inactive examples, the term \"as\" is being used in different contexts. For instance, it may be used to mean \"such as,\" \"as well as,\" or \"so long as,\" where it does not indicate an approximation, range, or comparison in relation to the described aspect.\n",
      "contains a phrase in which \"as\" is followed by an informative descriptive word or phrase, often indicating a comparison, a condition, or a reference to a specific category or example. However, the activation of this attention head may be less consistent than previously suggested.In the active examples, we can see \"as\" is followed by words like \"such as\" or \"as well as,\" but also by phrases where \"as\" is used alongside a conjunction, like \"As the moderators as.\" These phrases still give additional context or information about the preceding text.In the inactive examples, the attention head is not active even when \"as\" is followed by an informative phrase. It appears that this attention head may not play a significant role in some instances where \"as\" is introducing specific examples or categories. Furthermore, some inactive examples also include cases where \"as\" is used to indicate a temporal relationship (e.g., \"as soon as,\" \"as early as\") or comparisons (\"as slow as\n",
      "mentions an upcoming or future issue of a publication, such as a magazine, academic journal, newsletter, or similar items. It seems to be focused on recognizing the context of anticipation or expectation for a publication's new issue and predicting the word \"issue\" in those cases. In the inactive examples, the word \"issue\" is used in different contexts, such as collections of past or vintage issues, events related to specific issues, or non-publication matters like debates, manufacturing problems, or legal disputes, which the attention head does not focus on.\n",
      "mentions the term \"issue\" in various contexts, including publications and distributions as well as matters of concern, discussion, or problems that need to be resolved. In the active examples, it covers situations ranging from releasing new shares in the stock market to examining faulty equipment and addressing legal cases. In the inactive examples, the term \"issue\" is used in contexts like announcing new editions of magazines or newspapers, minting commemorative coins, and collaborations in designing cover art. Therefore, the attention head is not strictly limited to publication or dissemination contexts, but rather focuses on a broader range of scenarios where the term \"issue\" is used, encompassing both physical and non-physical issues.\n",
      "refers to an issuance or release of something periodic, such as a magazine edition, an anniversary issue, or a specific issue of a publication, when the document refers to a system or institution that has the obligation or authority to issue something, such as visas or financial services, or when the document discusses the topic of a specific problem, concern, or matter that needs to be addressed, solved, or debated. In the inactive examples, the context is not focused on a specific periodic issuance, mentions of entities with the obligation to issue something, or discussions of specific problems or concerns, but rather the documents touch on general events, historical context, or legalities that don't directly highlight the issuance concept.\n",
      "contains a context or situation where a process, authority, or entity is responsible for issuing something, discussing an issue related to an organization, or planning or preparing for a publication. The active examples demonstrate issuing in terms of visas, subscriptions, judgments, publications, consumer rights, decentralized networks and various contexts related to planning or discussing issues. In contrast, the inactive examples primarily involve the mention of specific issue numbers, individual instances or collections where the term \"issue\" is not directly associated with a process or entity responsible for the issuance, or primarily about the content of the issue itself.\n",
      "contains phrases related to closing or ending actions, events, or situations. The attention head helps the model to predict the next token as \"close\" when it appears in various contexts, such as closing actions (e.g., \"Press the button and wait for the gate to close\"), ending events (e.g., \"The conference call ended [...] the line would close\"), or approaching endings (e.g., \"knowing that the summit was close\").However, the attention head is not consistent in distinguishing between different contexts of the word \"close.\" In some cases, it becomes inactive when it should have been active, and vice versa. This might be due to the limitations of the attention head's ability to handle the complexity of the language and the wide range of contexts in which the word \"close\" can appear.\n",
      "mentions or implies a specific action or mechanism related to opening and closing physical objects, typically with a clear step-by-step description or an instructional tone. The attention head does not seem to activate when the document contents refer to \"close\" in the context of finishing, ending an event, or shutting down, or when the opening and closing actions are more metaphorical or ambiguous.\n",
      "discusses opening and closing actions or events that involve physical objects or digital content, particularly when there is a direct relationship between the two or when they are mentioned in close proximity. This could include opening and closing doors, browser tabs, windows, checkout aisles, or cages, as well as trades or orders. In the inactive examples, the context of \"close\" is either unrelated to an opening action, referring to a point in time or an end, or the opening action is not mentioned within the same sentence or context. This attention head may not consistently activate when dealing with non-physical opening and closing actions, such as in software interfaces, games, or workshops.\n",
      "contains a situation that is contingent on a specific condition or circumstance for the event \"won\" to occur. The active examples often involve an \"if\" clause or depend on a future uncertain outcome. On the other hand, the inactive examples describe events where the outcome is already determined, and the victory or achievement in question has been accomplished without any conditions or uncertainty involved.\n",
      "contains a phrase or sentence that refers to a situation, action, or event where the presented context suggests an unlikely or unexpected outcome leading to the prediction of a win. The active examples have sentences with either negative expressions, conditions, or surprising results, which imply initial uncertainty or doubt before the outcome occurs. In the inactive examples, the context does not express the same level of initial uncertainty or doubt, making the attention head inactive.\n",
      "contains a situation, condition, or reason that leads to a negative outcome or an inability for something to happen. In the active examples, there are specific circumstances or reasons that cause the subject or action not to work, or to fail, which is often followed by the word \"won.\" On the other hand, in the inactive examples, the word \"won\" is used in a context of competition or achievement without any explicit condition or situation implying that something will not happen. The attention head focuses on identifying situations or conditions that lead to the negative outcome denoted by \"won\" or on recognizing situations where \"won\" indicates a positive achievement or victory.\n",
      "contains a description or reference to a high level or state of something, often involving physical height, extreme actions or situations, or heightened emotions. The attention head is triggered by contextual clues that indicate the word \"high\" is referring to the intensity, elevation, or degree of a subject, such as adjectives like \"awfully\", \"dangerously\", or phrases like \"held high\" and \"leapt to\". The attention head is less active when the word \"high\" is part of a compound term, like \"high school\", or when \"high\" is part of a larger sentence without emphasis on it as a keyword. In some cases, the attention head may not activate for situations involving metaphorical heights or high levels, such as in company success or ranking achievements.\n",
      "contains a phrase where \"high\" is used in various contexts, not limited to having a positive or elevated quality or level, nor just a quantitative measure or a plain description. The attention head appears to be sensitive to the presence of \"high\" in a wide range of situations, including achieving goals, training, creativity, and even quantitative measures or compound nouns like \"high school diploma\" or \"Haryana high\". As the new evidence suggests, the attention head does not have a consistent pattern of activation in relation to the perceived elevated status, achievement or quality. Instead, it appears to respond to diverse uses of \"high,\" making its activation more unpredictable and generalized.\n",
      "contains the word \"very\" and is often part of, or immediately preceded by, a contextual phrase, such as \"at the very,\" \"by its very,\" \"to say the very,\" or \"in its very.\" In other words, the attention head is activated when the token (\"very\") contributes to a meaningful phrase in the context of the document, or when \"very\" is used to intensify adjectives or adverbs, as seen in the new active examples. The inactive examples have been reclassified as active, as they also fit these situations. Therefore, the attention head is inactive in very few or no cases with the given examples.\n",
      "contains the word \"very\" within a particular context or function that emphasizes the importance, extremity, or intensity of a situation, condition, or characteristic being discussed. Common phrases include \"at the very\", \"by its very\", \"by their very\", \"to say the very\", and \"from the very\". These phrases convey the idea of utmost significance or intensity of the subject at hand. The attention head is inactive when \"very\" is used as an adverb to emphasize the degree of an adjective or another adverb, often repeated twice, like \"very, very\". In these inactive instances, \"very\" modifies and intensifies the meaning of a subsequent word but does not necessarily convey the same level of importance or extremity as in the active examples.\n",
      "contains the word \"very\" as the last token, and it is part of a phrase or sentence expressing emphasis, intensity, or an extreme quality. However, the head is not limited to such usages anymore, as it now also seems to be active in more specific or technical contexts. The activation of the attention head is consistent across various domains, ranging from casual conversations to academic and professional discussions. It appears that the attention head is sensitive to instances where \"very\" is used to express a notable quality, attribute, or outcome, regardless of whether it is used for emphasis or as an intensifier. The only exception found is when \"very\" is used in the context of discussing material in a classroom setting, as seen in the inactive case with a teacher.\n",
      "discusses or describes a subject that is facing or overcoming challenges, showing growth, or adapting to novel situations. In active examples, the subjects often exhibit determination, resilience, or creativity in addressing their circumstances. Inactive examples tend to describe subjects' skills, attributes, or actions without highlighting their efforts or adaptability in the face of challenges or obstacles.\n",
      "focuses on a specific individual, usually male, and their actions, behavior, characteristics, or accomplishments. It is sensitive to content that highlights the person's achievements, talents, or impact in various areas, such as leadership, sports, arts, and technology. The attention head assists the model in predicting the next token as \"His\" by recognizing the context related to that individual person. In the inactive examples, the document may discuss events, concepts, or objects unrelated to a specific person or their actions, or the content may involve more general ideas rather than focusing on an individual's characteristics or achievements.\n",
      "contains a reference to a person's action, behavior, thoughts, or characteristics, or is describing a relationship between individuals or their roles. It typically emphasizes the individual's attributes, motivations, or a specific aspect of their personality or actions, but can also activate in situations where the context revolves around someone's position or connection to another person. In the inactive examples, the context tends to involve a more introspective or independent focus on the character's experiences or actions without direct relation to others.\n",
      "focuses on describing an individual's skills, achievements, or characteristics, emphasizing their capabilities or expertise in a particular area. The usage of \"His\" in this context suggests that the text will provide further information or elaboration concerning the individual's abilities or accomplishments. In the inactive examples, the context of \"His\" does not pertain to an emphasis on a person's skills or achievements but is instead connected to a different aspect of the situation or unrelated to a specific person.\n",
      "contains certain phrases, structures, or contexts, but it does not exclusively predict that the next token would be \"of.\" The attention head appears to be sensitive to a broader set of linguistic patterns and cues, and it may not solely focus on predicting the last token as \"of.\" In both active and inactive examples, the attention head can be engaged in various scenarios, including formal or legal contexts, everyday situations, or personal experiences.It is possible that the attention head is designed to recognize specific patterns or word combinations and then use that information to help predict the subsequent token, whether it is \"of\" or another word. This suggests that the attention head is more versatile and adaptive than initially assumed, and its primary function may not be solely focused on predicting the occurrence of the token \"of.\"As a result, the attention head can still provide valuable information and contribute to the overall performance of the neural network, even if it does not have a specific target token to predict. Instead, it may play a\n",
      "contains phrases that indicate a potential dispute or conflict resolution scenario, such as \"arbitration,\" \"neutral arbitrator,\" or \"in accordance with the rules.\" In these situations, the attention head may anticipate the mention of specific rules or precedents, prompting the prediction of \"of.\" In contrast, the inactive examples tend to discuss various topics without a focus on disputes or conflict resolution, including software licenses, general information, or personal experiences, making this attention head unresponsive in such contexts.\n",
      "contains a word or phrase that is either abbreviated, truncated, or an incomplete term, and requires the last token \"\" to be appended in order to form the complete word or phrase. The attention head recognizes patterns of such incomplete terms present in the document and assists in predicting the completion of the term. It appears that this attention head is more sensitive to these incomplete terms than previously thought, as it is active in nearly all the provided examples, including ones where such abbreviations or truncations were not as evident. As a result, the attention head may not only be detecting obvious incomplete words or abbreviations but also cases where a word or phrase could potentially be completed with \"\" even though it might not be semantically accurate.\n",
      "contains a word or phrase that is either abbreviated, truncated, or an incomplete term, and requires the last token \"\" to be appended in order to form the complete word or phrase. The attention head recognizes patterns of such incomplete terms present in the document and assists in predicting the completion of the term. It appears that this attention head is more sensitive to these incomplete terms than previously thought, as it is active in nearly all the provided examples, including ones where such abbreviations or truncations were not as evident. As a result, the attention head may not only be detecting obvious incomplete words or abbreviations but also cases where a word or phrase could potentially be completed with \"\" even though it might not be semantically accurate.\n",
      "contains specific keywords or phrases related to complex processes, techniques, or specialized contexts, such as \"secondary metabolism,\" \"cell signaling,\" \"mycotoxin metabolism,\" and \"immune system.\" It also seems to be active in some instances where specialized techniques are mentioned in the context of sports, cooking, and software development. In contrast, the attention head is inactive in documents with simpler, more everyday contexts like grocery shopping, apartment buildings, or enjoying music. It appears that the attention head is focusing on the presence of specialized terms or complex concepts to predict the last token as \"\".\n",
      "contains specific details, situations, or actions leading up to a point where \"The\" would be the natural next token to introduce a new piece of information or a continuation of the story. In active examples, the attention head identifies a context where adding \"The\" provides a coherent link to a subsequent event or fact. Meanwhile, in inactive examples, the document either provides general statements or actions that do not necessarily require \"The\" as the next token, or the addition of \"The\" would not contribute to the coherence or progression of the text.\n",
      "contains a phrase or a sentence that appears to be naturally followed by \"The\" but may not necessarily have a specific context or coherent continuation. It detects instances where \"The\" can potentially be the next token but does not focus on the coherence or completeness of the preceding context.\n",
      "contains a sentence structure or pattern that typically precedes \"The\" in texts. However, it does not seem to be specifically tied to the presence of a noun or subject. In the new evidence, the attention head is inactive in all examples, including those that were previously categorized as active. This might indicate that the attention head is less focused on identifying a contextual subject or topic and more focused on the syntactical or grammatical pattern leading up to \"The.\"\n",
      "contains a phrase or a sentence where \"The\" is preceded by a preposition or a conjunction, indicating that \"The\" is at the beginning of a new clause or a separate action. In this case, \"The\" acts as a natural continuation from the previous context. In contrast, the attention head remains inactive when the document had a proper ending just before \"The,\" as no additional clause or action is expected.\n",
      "contains the word \"power\" as the last token and has a broad or general meaning associated with it, such as influence, control, or energy. In the active cases, \"power\" is often used in contexts where it is the main focus or concern. In the inactive cases, \"power\" appears alongside other words like \"inner power,\" \"political power,\" \"physical power,\" \"superpower,\" or as a part of compound words such as \"processing power\" and \"stopping power,\" which are more specific and narrow in meaning.\n",
      "contains the word \"power\" as the last token and has a context that revolves around authority, influence, control, or various forms of energy and electricity. It is not limited to just political and authoritative contexts but also includes instances where \"power\" is used to describe energy sources, such as electric, wind, or solar power. However, it doesn't activate when \"power\" is part of a phrase or within a quote where it doesn't represent the main topic of the document. Additionally, it doesn't activate when \"power\" is used in the context of processing capabilities, physical strength, or energy conservation tips.\n"
     ]
    }
   ],
   "source": [
    "# Then run it a second time\n",
    "all_neuron_heads = list(neuron_head_to_response.keys())\n",
    "\n",
    "generation_prompts_v2 = {}\n",
    "\n",
    "for neuron_head in all_neuron_heads:\n",
    "    explanation_str = neuron_head_to_response[neuron_head].replace(\"\\n\", \"\")\n",
    "    try:\n",
    "        explanation_str = explanation_str.split(\"</explanation>\")[0]\n",
    "        print(explanation_str)\n",
    "    except:\n",
    "        continue\n",
    "    token = neuron_to_token[(neuron_head[0], neuron_head[1])]\n",
    "    prompt_gen = GenerationPrompt(explanation_str.replace(\"\\n\", \"\"), token)\n",
    "    gpt_4_prompt = prompt_gen.get_prompt(num_examples=10, generate_negative=True)\n",
    "    generation_prompts_v2[neuron_head] = gpt_4_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are studying attention heads in a transformer architecture neural network. Each attention head looks for some particular thing in a short document.\n",
      "The attention head being studied helps to predict that the next token is \" number\", but it is only active in some documents and not others.\n",
      "Explanation: \"\"\"\n",
      "In particular, this attention head is active when the document contains a context or phrase that is related to record-breaking, chart-topping, or being highly ranked in terms of popularity or achievement, often in the music domain. The attention head is inactive when the document contains numbers related to rankings, positions, or sequences in other domains, as well as when it involves identification, phone numbers, parts, approvals, or other non-ranking contexts. The active examples usually involve quantifying the success or popularity of a subject, entity, or performance.</explanation>With the explanation, the reasoner categorizes the following examples of documents that activate this attention head:Examples where the attention head is active: \"\"\"*The music industry announced their annual Billboard chart results. The pop sensation's latest song reached number\"\"\"Examples where the attention head is inactive: \"\"\"*The athletes competed fiercely in the race. John came in at number*The chess tournament was a battleground of intellects. After several nerve-wracking matches, Mary emerged as the number*The\n",
      "\"\"\"\n",
      "With this explanation, generate 10 examples of documents that activate this attention head, and 10 examples of documents that do not activate this attention head.\n",
      "The documents should be 2 sentences long, and the token \" number\" should appear in the second sentence.\n",
      "\n",
      "Desired Format:\n",
      "Examples where the attention head is active:\n",
      "1. <example_1>\n",
      "2. <example_2>\n",
      "...\n",
      "\\Examples where the attention head is inactive:\n",
      "1. <example_1>\n",
      "2. <example_2>\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i+=1\n",
    "print(list(generation_prompts_v2.values())[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = \"data/eg_generation_iter_1.jsonl\"\n",
    "jobs = [\n",
    "            {\"model\":\"gpt-4\",\n",
    "            \"messages\":[{\"role\": \"user\", \"content\": gpt_4_prompt}],\n",
    "            \"max_tokens\":800,\n",
    "        } for gpt_4_prompt in generation_prompts_v2.values()]\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for job in jobs:\n",
    "        json_string = json.dumps(job)\n",
    "        f.write(json_string + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "now = int(time.time())\n",
    "print(now)\n",
    "\n",
    "#########################################################################################\n",
    "# WHEN YOU UPDATE THE CELL, REMEMBER TO UPDATE THE JSONL FILE NAME IF YOU'VE CHANGED IT #\n",
    "#########################################################################################\n",
    "current_time = 1686792857\n",
    "\n",
    "if current_time + 20 < now: # Sanity check to make sure you don't spam this cell\n",
    "    raise Exception(\"Update the current_time variable to be able to run this cell! Copy and paste the number above.\")\n",
    "else:\n",
    "    print(\"all gucci\")\n",
    "    !python3 api_request_parallel_processor.py --requests_filepath data/eg_generation_iter_1.jsonl --request_url https://api.openai.com/v1/chat/completions --max_requests_per_minute 100 --max_tokens_per_minute 20000\n",
    "    # ^ This is very scary because the stdout looks like it sends repeated requests for the same thing so just run it in terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results and make the {neuron_head: response} dictionary\n",
    "\n",
    "with open('data/eg_generation_iter_1_results.jsonl', 'r') as json_file:\n",
    "    raw_explanations = list(json_file)\n",
    "\n",
    "prompt_to_response = {}\n",
    "neuron_head_to_response = {}\n",
    "neuron_head_to_prompt = generation_prompts_v2\n",
    "neuron_heads = list(neuron_head_to_prompt.keys())\n",
    "\n",
    "for i, json_str in enumerate(raw_explanations):\n",
    "    result = json.loads(json_str)\n",
    "    prompt = result[0][\"messages\"][0][\"content\"]\n",
    "    response = result[1][\"choices\"][0][\"message\"][\"content\"]\n",
    "    prompt_to_response[prompt] = response\n",
    "\n",
    "for neuron_head in neuron_heads:\n",
    "    prompt = neuron_head_to_prompt[neuron_head]\n",
    "    response = prompt_to_response[prompt]\n",
    "    neuron_head_to_response[neuron_head] = response\n",
    "\n",
    "\n",
    "with open(\"data/eg_generation_iter_1_nh_to_exp.json\", \"w\") as f:\n",
    "    nh_to_response_str_key = {str(k):v for k,v in neuron_head_to_response.items()}\n",
    "    json.dump(nh_to_response_str_key, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:07,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:10,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:12,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:15,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:18,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:22,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:25,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:30,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:35,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:43,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:55,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [01:08,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:15,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [01:19,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [01:23,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [01:28,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [01:30,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [01:34,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [01:38,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [01:40,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [01:45,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [01:50,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [01:54,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [01:57,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [02:02,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [02:05,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [02:06,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [02:10,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [02:13,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [02:18,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [02:21,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [02:25,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [02:27,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [02:30,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [02:32,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [02:33,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [02:34,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [02:36,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [02:38,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [02:41,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [02:45,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [02:51,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [02:57,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [02:59,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [03:01,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [03:02,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [03:03,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [03:07,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [03:09,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [03:12,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [03:16,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [03:22,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [03:28,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [03:32,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [03:37,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [03:40,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [03:45,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [03:47,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [03:48,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [03:53,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [03:56,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [03:57,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [04:02,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [04:05,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [04:06,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [04:08,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [04:12,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [04:17,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [04:22,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [04:24,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [04:25,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [04:27,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [04:29,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [04:30,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [04:31,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [04:35,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [04:42,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [04:46,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [04:50,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [04:55,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82it [04:58,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [05:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [05:05,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [05:10,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [05:15,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87it [05:17,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [05:20,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [05:22,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [05:23,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [05:27,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [05:33,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93it [05:36,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [05:37,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [05:39,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [05:40,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97it [05:44,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [05:47,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [05:51,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [05:53,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [05:54,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [05:58,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [05:59,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [06:04,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [06:07,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106it [06:09,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "107it [06:10,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [06:11,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109it [06:12,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [06:18,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [06:21,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [06:22,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [06:24,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114it [06:26,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115it [06:31,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116it [06:37,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117it [06:43,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [06:49,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119it [06:55,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [06:58,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [07:03,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "122it [07:07,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [07:13,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124it [07:19,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [07:24,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126it [07:30,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [07:32,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128it [07:36,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [07:41,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [07:47,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "131it [07:50,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "132it [07:55,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "133it [08:02,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [08:05,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "135it [08:06,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "136it [08:08,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "137it [08:09,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138it [08:11,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "139it [08:13,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [08:16,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141it [08:19,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "142it [08:23,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "143it [08:29,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [08:33,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [08:34,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146it [08:36,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "147it [08:40,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [08:44,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "149it [08:49,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [08:55,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "151it [09:01,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [09:06,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "153it [09:10,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "154it [09:14,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "155it [09:19,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [09:22,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [09:26,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158it [09:31,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159it [09:34,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [09:35,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [09:37,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "162it [09:38,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "163it [09:39,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [09:40,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:41,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [09:42,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [09:48,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168it [10:00,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169it [10:13,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170it [10:23,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "171it [10:26,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [10:29,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n",
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "174it [10:30,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "175it [10:31,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [10:35,  3.61s/it]\n"
     ]
    }
   ],
   "source": [
    "check_if_responses_can_be_processed = False # Do this first\n",
    "iteration_prompts = {}\n",
    "iteration_prompts_ref = {}\n",
    "\n",
    "for i, (neuron_head, generated_example) in tqdm(enumerate(neuron_head_to_response.items())):\n",
    "    neuron, head_no = (neuron_head[0], neuron_head[1]), neuron_head[2]\n",
    "    relevant_token = neuron_to_token[neuron]\n",
    "\n",
    "    capitalised_token = relevant_token[0] + relevant_token[1].capitalize() + relevant_token[2:] # we can do this because by definition we got \" word\" tokens\n",
    "    \n",
    "    if 'is_inactive' in generated_example:\n",
    "        generated_example = generated_example.replace('is_inactive', 'is inactive') # For some reason this was in one prompt\n",
    "\n",
    "    # Extract the positive and negative examples. \n",
    "    def get_list_of_strings(string):\n",
    "        return [s[3:] if s[1] == \".\" else s[4:] for s in string.split(\"\\n\") if s != ''] # Wonky but it works\n",
    "\n",
    "    positive_examples = generated_example.split(\"Examples where the attention head is active:\")[1].split(\"Examples where the attention head is inactive:\")[0]\n",
    "    negative_examples = generated_example.split(\"Examples where the attention head is inactive:\")[1]\n",
    "    positive_examples = get_list_of_strings(positive_examples)\n",
    "    negative_examples = get_list_of_strings(negative_examples)\n",
    "\n",
    "    # Cut off the examples at the last instance of <token>. They may happen more than once.\n",
    "    positive_examples = [eg[:max(eg.rfind(relevant_token), eg.rfind(capitalised_token))] for eg in positive_examples]\n",
    "    negative_examples = [eg[:max(eg.rfind(relevant_token), eg.rfind(capitalised_token))] for eg in negative_examples]\n",
    "\n",
    "    if not check_if_responses_can_be_processed:\n",
    "        # Load and Truncate Prompts (slightly cursed way of string -> token -> truncated tokens -> string)\n",
    "        trunc_prompts = positive_examples + negative_examples\n",
    "        # Run head attribution\n",
    "        tokens = model.to_tokens(trunc_prompts, prepend_bos=True).to(device=device)\n",
    "        original_logits, cache = model.run_with_cache(tokens, )\n",
    "\n",
    "        # # Prepare prompts by heads\n",
    "        head_attribution = get_head_attribution(model, cache, tokens, neuron)\n",
    "        _, top_heads = torch.topk(head_attribution, k=3, dim=-1)\n",
    "        top_heads_list = top_heads.tolist()\n",
    "\n",
    "        top_heads_positive = top_heads_list[:len(positive_examples)]\n",
    "        top_heads_negative = top_heads_list[len(positive_examples):]\n",
    "\n",
    "        positive_examples_correct = []\n",
    "        negative_examples_correct = []\n",
    "\n",
    "        for i, example in enumerate(positive_examples):\n",
    "            if head_no in top_heads_positive[i]:\n",
    "                positive_examples_correct.append(example)\n",
    "            else:\n",
    "                negative_examples_correct.append(example)\n",
    "\n",
    "        for i, example in enumerate(negative_examples):\n",
    "            if head_no in top_heads_negative[i]:\n",
    "                positive_examples_correct.append(example)\n",
    "            else:\n",
    "                negative_examples_correct.append(example)\n",
    "\n",
    "        explanation_str = nh_to_exp[neuron_head].replace(\"\\n\", \"\")\n",
    "        iter_gen = IterationPromptGen(positive_examples, negative_examples, positive_examples_correct, negative_examples_correct, relevant_token, explanation_str)\n",
    "        iteration_prompt = iter_gen.get_prompt()\n",
    "\n",
    "        iteration_prompts[neuron_head] = iteration_prompt\n",
    "        iteration_prompts_ref[neuron_head] = [positive_examples, negative_examples, positive_examples_correct, negative_examples_correct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/generated_prompts_v2_1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(iteration_prompts_ref, f)\n",
    "\n",
    "# ref is original, ref_1 is with token at end of example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/generated_prompts_v2_1.pkl\", \"rb\") as f:\n",
    "    v2 = pickle.load(f)\n",
    "\n",
    "with open(\"data/iteration_prompts_ref_1.pkl\", \"rb\") as f:\n",
    "    v1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5954229797979799\n",
      "0.4034090909090909\n",
      "0.7856534090909091\n",
      "0.40573377829807455\n",
      "0.4034090909090909\n",
      "0.6660497535647056\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "accuracies_positive = {}\n",
    "accuracies_negative = []\n",
    "f1s = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "for head, (pred_pos, pred_neg, correct_pos, correct_neg) in v2.items():\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    for eg in pred_pos:\n",
    "        if eg in correct_pos:\n",
    "            true_pos += 1\n",
    "        else:\n",
    "            false_pos += 1\n",
    "    for eg in pred_neg:\n",
    "        if eg in correct_neg:\n",
    "            true_neg += 1\n",
    "        else:\n",
    "            false_neg += 1\n",
    "    accuracy = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
    "    accuracy_positive = true_pos / (true_pos + false_pos)\n",
    "    accuracy_negative = true_neg / (true_neg + false_neg)\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    try:\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "        recalls.append(recall)\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    precisions.append(precision)\n",
    "\n",
    "    # accuracies_positive.append(accuracy_positive)\n",
    "    accuracies_positive[head] = accuracy_positive\n",
    "    accuracies_negative.append(accuracy_negative)\n",
    "    accuracies.append(accuracy)\n",
    "    f1 = 2 * true_pos / (2 * true_pos + false_pos + false_neg)\n",
    "    f1s.append(f1)\n",
    "\n",
    "\n",
    "\n",
    "print(np.mean(accuracies))\n",
    "print(np.mean(list(accuracies_positive.values())))\n",
    "print(np.mean(accuracies_negative))\n",
    "print(np.mean(f1s))\n",
    "print(np.mean(precisions))\n",
    "print(np.mean(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "<b>%{hovertext}</b><br><br>=%{y}<extra></extra>",
         "hovertext": [
          " only",
          " only",
          " only",
          " only",
          " only",
          " number",
          " number",
          " number",
          " go",
          " go",
          " go",
          " go",
          " go",
          " together",
          " together",
          " together",
          " together",
          " together",
          " together",
          " called",
          " called",
          " called",
          " called",
          " called",
          " called",
          " first",
          " first",
          " first",
          " used",
          " used",
          " used",
          " used",
          " within",
          " within",
          " within",
          " way",
          " way",
          " way",
          " way",
          " way",
          " out",
          " out",
          " out",
          " too",
          " too",
          " too",
          " will",
          " will",
          " right",
          " right",
          " right",
          " over",
          " over",
          " over",
          " out",
          " out",
          " out",
          " once",
          " once",
          " her",
          " her",
          " her",
          " her",
          " class",
          " class",
          " class",
          " class",
          " use",
          " use",
          " use",
          " use",
          " much",
          " much",
          " much",
          " much",
          " so",
          " so",
          " so",
          " so",
          " state",
          " state",
          " state",
          " state",
          " RandomRedditor",
          " RandomRedditor",
          " about",
          " about",
          " about",
          " about",
          " about",
          " following",
          " following",
          " following",
          " following",
          " following",
          " of",
          " of",
          " after",
          " after",
          " after",
          " after",
          " after",
          " around",
          " around",
          " around",
          " last",
          " last",
          " last",
          " last",
          " by",
          " by",
          " by",
          " off",
          " off",
          " off",
          " off",
          " down",
          " down",
          " down",
          " down",
          " down",
          " back",
          " back",
          " well",
          " there",
          " there",
          " there",
          " up",
          " up",
          " up",
          " no",
          " no",
          " no",
          " no",
          " after",
          " after",
          " after",
          " after",
          " after",
          " under",
          " under",
          " under",
          " as",
          " as",
          " as",
          " as",
          " issue",
          " issue",
          " issue",
          " issue",
          " close",
          " close",
          " close",
          " won",
          " won",
          " won",
          " high",
          " high",
          " very",
          " very",
          " very",
          " His",
          " His",
          " His",
          " His",
          " of",
          " of",
          " ",
          " ",
          " ",
          " The",
          " The",
          " The",
          " The",
          " power",
          " power"
         ],
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175
         ],
         "xaxis": "x",
         "y": [
          0,
          0,
          1,
          0.6,
          0.6,
          0.2,
          0.9,
          0.3,
          0,
          0,
          0,
          0,
          0,
          0.9,
          0.3,
          0.1,
          0.7,
          0.9,
          0.5,
          0.6,
          0,
          0.2,
          0.5,
          0.1,
          0.2,
          0.2,
          0.8,
          0.3,
          0,
          0,
          0.9,
          0.3,
          1,
          0.8,
          0.3,
          0.3,
          0,
          0.8,
          0.5,
          0.3,
          0.4,
          1,
          0.6,
          0.4,
          0.2,
          1,
          1,
          1,
          0.1,
          0,
          0,
          1,
          1,
          0.7,
          1,
          0.6,
          0.8,
          0,
          0,
          0.4,
          0.7,
          0.9,
          0.4,
          0.9,
          0.7,
          0.4,
          0.6,
          0,
          0.6,
          0.5,
          0.4,
          0.1,
          0.1,
          0.3,
          0,
          0,
          1,
          0,
          0,
          0.9,
          0.1,
          0,
          0.8,
          0,
          0.9,
          0,
          0,
          0,
          0.6,
          0.3,
          0.5,
          0,
          0.8,
          1,
          0.9,
          0.4,
          0.1,
          0,
          0,
          0.1,
          0.5,
          0.3,
          0.2,
          0.5,
          0.1,
          0.7,
          0,
          0.6,
          0,
          0.2,
          0.5,
          0,
          0,
          0,
          0,
          0.9,
          0,
          0,
          0.1,
          0.7,
          1,
          0.6,
          0,
          0.4,
          0,
          0,
          0,
          0.8,
          0,
          0,
          1,
          0.2,
          0.9,
          0,
          0.9,
          0.4,
          0,
          0,
          0.4,
          0.3,
          1,
          0,
          0,
          0,
          1,
          0.5,
          0.8,
          1,
          0.5,
          0.9,
          0.1,
          0.2,
          0.5,
          0.6,
          0,
          0.2,
          0.8,
          0,
          1,
          1,
          1,
          0.4,
          1,
          0.7,
          0.6,
          0,
          0.2,
          0.9,
          0.7,
          0.2,
          1,
          0,
          0,
          0,
          0.4,
          0.3
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(np.arange(len(accuracies_positive)), list(accuracies_positive.values()), hover_name=labels) # add hover"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
