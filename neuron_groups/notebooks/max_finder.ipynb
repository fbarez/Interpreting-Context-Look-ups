{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook\n"
     ]
    }
   ],
   "source": [
    "import plotly.io as pio\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"Running as a Colab notebook\")\n",
    "    pio.renderers.default = \"colab\"\n",
    "    %pip install transformer-lens fancy-einsum\n",
    "    %pip install -U kaleido # kaleido only works if you restart the runtime. Required to write figures to disk (final cell)\n",
    "except:\n",
    "    print(\"Running as a Jupyter notebook\")\n",
    "    pio.renderers.default = \"vscode\"\n",
    "    from IPython import get_ipython\n",
    "    ipython = get_ipython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from fancy_einsum import einsum\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, utils, ActivationCache\n",
    "from torchtyping import TensorType as TT\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import einops\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # https://stackoverflow.com/q/62691279\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: circuitsvis in /Users/clementneo/projects/aan/aan/lib/python3.10/site-packages (1.39.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.23 in /Users/clementneo/projects/aan/aan/lib/python3.10/site-packages (from circuitsvis) (1.24.1)\n",
      "Requirement already satisfied: torch<2.0,>=1.10 in /Users/clementneo/projects/aan/aan/lib/python3.10/site-packages (from circuitsvis) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /Users/clementneo/projects/aan/aan/lib/python3.10/site-packages (from circuitsvis) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/clementneo/projects/aan/aan/lib/python3.10/site-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/clementneo/projects/aan/aan/lib/python3.10/site-packages (from torch<2.0,>=1.10->circuitsvis) (4.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install circuitsvis\n",
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.renderers.default='vscode'\n",
    "\n",
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-large into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-large\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Activating Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration NeelNanda--pile-10k-72f566e9f7c464ab\n",
      "Found cached dataset parquet (/Users/clementneo/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"NeelNanda/pile-10k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13274\n",
      "torch.Size([5, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.to_dict()['text'][0]))\n",
    "dataset_text_list = dataset.to_dict()['text']\n",
    "print(model.to_tokens(dataset.to_dict()['text'][0:5]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(31, 3621), (31, 364), (31, 2918), (31, 4378), (31, 988), (31, 2658), (31, 2692), (31, 4941), (31, 2415), (31, 1407), (32, 4964), (32, 2412), (32, 4282), (32, 3151), (32, 1155), (32, 1386), (32, 3582), (32, 4882), (32, 3477), (32, 406), (33, 1202), (33, 524), (33, 1582), (33, 4446), (33, 204), (33, 4900), (33, 2322), (33, 3278), (33, 1299), (33, 52), (34, 4012), (34, 4262), (34, 320), (34, 5095), (34, 2599), (34, 2442), (34, 4494), (34, 4199), (34, 727), (34, 4410), (35, 4518), (35, 48), (35, 5014), (35, 3724), (35, 3360), (35, 885), (35, 4924), (35, 274), (35, 2369), (35, 4638)]\n"
     ]
    }
   ],
   "source": [
    "neurons_json = json.load(open(\"neuron_finder_results.json\"))\n",
    "neurons = []\n",
    "for layer, results in neurons_json.items():\n",
    "    indices = results.keys()\n",
    "    for index in indices:\n",
    "        neurons.append((int(layer), int(index)))\n",
    "\n",
    "print(neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neurons_acts(model, texts, neurons):\n",
    "    tokens = model.to_tokens(texts)\n",
    "    \n",
    "def cache_to_tuples(cache):\n",
    "    new_cache = {}\n",
    "    for key in cache.keys():\n",
    "        x = torch.max(cache[key], dim=1)\n",
    "        y = list(x)\n",
    "        y = [y[0].tolist(), y[1].tolist()]\n",
    "        y = list(zip(*y))\n",
    "        new_cache[key] = y # y is a list of tuples, i.e. [(max_value, max_index), ...]\n",
    "    return new_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('blocks.0.mlp.hook_post', <function return_caching_hook.<locals>.caching_hook at 0x2974e0160>), ('blocks.1.mlp.hook_post', <function return_caching_hook.<locals>.caching_hook at 0x2c775caf0>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:33<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "batched_texts = [dataset_text_list[i: i+batch_size] for i in range(0, len(dataset_text_list), batch_size)]\n",
    "print(len(batched_texts))\n",
    "\n",
    "neuron_max_acts = {neuron: [] for neuron in neurons}\n",
    "\n",
    "for texts in tqdm(batched_texts):\n",
    "    model.reset_hooks()\n",
    "\n",
    "    cache = {}\n",
    "\n",
    "    def return_caching_hook(neuron):\n",
    "        layer, neuron_index = neuron\n",
    "        def caching_hook(act, hook):\n",
    "            cache[(layer, neuron_index)] = act[:, :, neuron_index] # act shape is (batch_size, seq_len, neuron_index)\n",
    "        return caching_hook\n",
    "    \n",
    "    hooks = list(((f\"blocks.{layer}.mlp.hook_post\", return_caching_hook((layer, index))) for layer, index in neurons))\n",
    "    print(hooks)\n",
    "\n",
    "    model.run_with_hooks(\n",
    "        model.to_tokens(texts),\n",
    "        fwd_hooks=hooks,\n",
    "    )\n",
    "    cache = cache_to_tuples(cache)\n",
    "\n",
    "    for key in cache.keys():\n",
    "        neuron_max_acts[key].extend(cache[key])\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): [(0.4795704782009125, 488), (1.505552887916565, 152), (1.5584756135940552, 137), (-0.0, 22)], (1, 1): [(-0.0, 566), (0.20460055768489838, 810), (2.280916929244995, 76), (0.30561938881874084, 66)]}\n"
     ]
    }
   ],
   "source": [
    "print(neuron_max_acts)\n",
    "with open(\"neuron_max_acts.json\", \"w\") as f:\n",
    "    json.dump(neuron_max_acts, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
