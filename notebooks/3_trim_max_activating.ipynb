{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scipy 1.9.2 requires numpy<1.26.0,>=1.18.5, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformer_lens -q\n",
    "!pip install --upgrade attrs -q\n",
    "!pip uninstall jax jaxlib -y -q\n",
    "!pip install jax jaxlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "from src.neuron_texts import find_truncated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load jsons from ../experiment_data/1_next_token_neurons\n",
    "# filename = \"2023-10-14_17-23-35_gpt2-large_mid_test\"\n",
    "# filename = \"2024-02-13_06-56-03_pythia-160m\"\n",
    "# filename = \"2024-02-13_07-47-52_pythia-410m\"\n",
    "# filename = \"2024-02-13_09-18-06_pythia-1.4b\"\n",
    "\n",
    "filename = \"2024-02-13_11-37-07_pythia-1.4b_test\"\n",
    "filename = \"2024-02-13_12-53-32_pythia-410m_test\"\n",
    "\n",
    "\n",
    "train = False\n",
    "\n",
    "with open(f'./experiment_data/2_max_activating_texts/{filename}.json') as f:\n",
    "    max_text_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/pty.py:85: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-11_14-06-30_gpt2-small.json\n",
      "2023-10-11_14-29-40_gpt2-medium.json\n",
      "2023-10-11_14-30-39_gpt2-medium.json\n",
      "2023-10-11_16-36-33_gpt2-large.json\n",
      "2023-10-13_03-52-29_gpt2-small_test.json\n",
      "2023-10-13_04-29-47_gpt2-medium_test.json\n",
      "2023-10-13_10-22-07_gpt2-large_test.json\n",
      "2023-10-13_15-31-49_gpt2-large_mid.json\n",
      "2023-10-13_19-47-54_gpt2-large_mid.json\n",
      "2023-10-14_17-23-35_gpt2-large_mid_test.json\n",
      "2024-02-13_06-56-03_pythia-160m.json\n",
      "2024-02-13_07-47-52_pythia-410m.json\n",
      "2024-02-13_09-18-06_pythia-1.4b.json\n",
      "2024-02-13_11-37-07_pythia-1.4b_test.json\n",
      "2024-02-13_12-53-32_pythia-410m_test.json\n",
      "2024-02-13_13-17-59_pythia-160m_test.json\n"
     ]
    }
   ],
   "source": [
    "!ls experiment_data/2_max_activating_texts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "parameters = max_text_data['parameters']\n",
    "model_name = parameters['model_name']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57dad0926774b52881b6b8b16ee0f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5989cb5f87fc4824b48168f9fa56f4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367475858d084c2eb90ebc7157b5a3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05796bf1a3de45ba93563296274f0585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7be8c6e19647bf99992457c72c7435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-1.4b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loaded!\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "    dataset_text_list = [x['text'] for x in dataset]\n",
    "    print(\"Train loaded!\")\n",
    "\n",
    "else:\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    with open('./text_list_dict_test.pkl', 'rb') as f:\n",
    "        dataset_text_dict = pickle.load(f)\n",
    "\n",
    "    dataset_text_list = [x['text'] for x in dataset_text_dict]\n",
    "    dataset = dataset_text_dict\n",
    "\n",
    "    print(\"test loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:25,  1.18it/s]00:00<?, ?it/s]\n",
      "30it [00:23,  1.25it/s]00:25<33:26, 25.40s/it]\n",
      "30it [00:28,  1.05it/s]00:49<31:57, 24.59s/it]\n",
      "30it [00:30,  1.02s/it]01:17<33:51, 26.38s/it]\n",
      "30it [00:30,  1.01s/it]01:48<35:30, 28.04s/it]\n",
      "30it [00:30,  1.03s/it]02:18<36:06, 28.89s/it]\n",
      "30it [00:22,  1.36it/s]02:49<36:28, 29.58s/it]\n",
      "30it [00:30,  1.03s/it]03:11<33:01, 27.14s/it]\n",
      "30it [00:23,  1.30it/s]03:42<34:00, 28.34s/it]\n",
      "30it [00:27,  1.10it/s]04:06<31:37, 26.72s/it]\n",
      "30it [00:30,  1.00s/it][04:33<31:21, 26.88s/it]\n",
      "30it [00:31,  1.06s/it][05:03<32:03, 27.88s/it]\n",
      "30it [00:26,  1.15it/s][05:35<32:56, 29.06s/it]\n",
      "30it [00:25,  1.17it/s][06:01<31:25, 28.15s/it]\n",
      "30it [00:26,  1.12it/s][06:26<30:07, 27.39s/it]\n",
      "30it [00:23,  1.25it/s][06:53<29:27, 27.20s/it]\n",
      "30it [00:30,  1.01s/it][07:17<27:57, 26.21s/it]\n",
      "30it [00:29,  1.02it/s][07:47<28:47, 27.42s/it]\n",
      "30it [00:30,  1.01s/it][08:17<28:56, 28.01s/it]\n",
      "30it [00:24,  1.20it/s][08:47<29:09, 28.67s/it]\n",
      "30it [00:27,  1.10it/s][09:12<27:33, 27.56s/it]\n",
      "30it [00:28,  1.05it/s][09:39<27:03, 27.51s/it]\n",
      "30it [00:31,  1.05s/it][10:08<26:56, 27.87s/it]\n",
      "30it [00:31,  1.05s/it][10:40<27:31, 28.98s/it]\n",
      "30it [00:27,  1.09it/s][11:11<27:46, 29.75s/it]\n",
      "30it [00:33,  1.10s/it][11:39<26:41, 29.12s/it]\n",
      "30it [00:34,  1.14s/it][12:12<27:16, 30.31s/it]\n",
      "30it [00:31,  1.06s/it][12:46<27:47, 31.46s/it]\n",
      "30it [00:30,  1.02s/it][13:18<27:20, 31.55s/it]\n",
      "30it [00:40,  1.35s/it][13:48<26:35, 31.29s/it]\n",
      "30it [00:33,  1.11s/it][14:29<28:20, 34.02s/it]\n",
      "30it [00:27,  1.10it/s][15:02<27:38, 33.84s/it]\n",
      "30it [00:32,  1.08s/it][15:30<25:31, 31.91s/it]\n",
      "30it [00:27,  1.07it/s][16:02<25:09, 32.12s/it]\n",
      "30it [00:34,  1.16s/it][16:30<23:40, 30.88s/it]\n",
      "30it [00:28,  1.05it/s][17:05<24:01, 32.04s/it]\n",
      "30it [00:28,  1.04it/s][17:33<22:42, 30.97s/it]\n",
      "30it [00:29,  1.00it/s][18:02<21:45, 30.36s/it]\n",
      "30it [00:27,  1.09it/s][18:32<21:10, 30.25s/it]\n",
      "30it [00:31,  1.06s/it][19:00<20:07, 29.44s/it]\n",
      "30it [00:31,  1.07s/it][19:32<20:05, 30.15s/it]\n",
      "30it [00:28,  1.04it/s][20:04<19:57, 30.71s/it]\n",
      "30it [00:31,  1.07s/it][20:33<19:06, 30.18s/it]\n",
      "30it [00:39,  1.30s/it][21:05<18:56, 30.72s/it]\n",
      "30it [00:34,  1.14s/it][21:44<19:56, 33.23s/it]\n",
      "30it [00:27,  1.11it/s][22:18<19:32, 33.51s/it]\n",
      "30it [00:25,  1.16it/s][22:45<17:53, 31.56s/it]\n",
      "30it [00:34,  1.14s/it][23:11<16:25, 29.87s/it]\n",
      "30it [00:34,  1.13s/it][23:45<16:38, 31.21s/it]\n",
      "30it [00:34,  1.16s/it][24:19<16:34, 32.07s/it]\n",
      "30it [00:32,  1.08s/it][24:54<16:26, 32.88s/it]\n",
      "30it [00:34,  1.16s/it][25:26<15:49, 32.73s/it]\n",
      "30it [00:34,  1.15s/it][26:01<15:35, 33.39s/it]\n",
      "30it [00:30,  1.00s/it][26:36<15:10, 33.74s/it]\n",
      "30it [00:57,  1.92s/it][27:06<14:08, 32.65s/it]\n",
      "30it [00:26,  1.15it/s][28:04<16:44, 40.17s/it]\n",
      "30it [00:32,  1.09s/it][28:30<14:22, 35.94s/it]\n",
      "30it [00:27,  1.09it/s][29:02<13:23, 34.95s/it]\n",
      "30it [00:25,  1.16it/s][29:30<11:59, 32.70s/it]\n",
      "30it [00:42,  1.40s/it][29:56<10:44, 30.68s/it]\n",
      "30it [00:31,  1.03s/it][30:38<11:23, 34.19s/it]\n",
      "30it [00:35,  1.20s/it][31:09<10:31, 33.25s/it]\n",
      "30it [00:26,  1.12it/s][31:45<10:13, 34.08s/it]\n",
      "30it [00:36,  1.22s/it][32:12<09:02, 31.90s/it]\n",
      "30it [00:29,  1.01it/s][32:49<08:53, 33.35s/it]\n",
      "30it [00:27,  1.08it/s][33:19<08:04, 32.27s/it]\n",
      "30it [00:37,  1.24s/it][33:46<07:12, 30.89s/it]\n",
      "30it [00:44,  1.49s/it][34:24<07:06, 32.80s/it]\n",
      "30it [00:42,  1.43s/it][35:08<07:16, 36.36s/it]\n",
      "30it [00:40,  1.37s/it][35:51<07:01, 38.31s/it]\n",
      "30it [00:35,  1.17s/it][36:32<06:31, 39.11s/it]\n",
      "30it [00:33,  1.12s/it][37:07<05:41, 37.90s/it]\n",
      "30it [00:33,  1.12s/it][37:41<04:52, 36.62s/it]\n",
      "30it [00:26,  1.15it/s][38:14<04:10, 35.74s/it]\n",
      "30it [00:27,  1.09it/s][38:41<03:17, 32.86s/it]\n",
      "30it [00:28,  1.04it/s][39:08<02:36, 31.23s/it]\n",
      "30it [00:30,  1.03s/it][39:37<02:02, 30.56s/it]\n",
      "30it [00:26,  1.14it/s][40:08<01:31, 30.64s/it]\n",
      "30it [00:23,  1.29it/s][40:34<00:58, 29.33s/it]\n",
      "30it [00:31,  1.05s/it][40:57<00:27, 27.50s/it]\n",
      "100%|██████████| 80/80 [41:29<00:00, 31.12s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    neuron_20_examples = find_truncated_texts(\n",
    "        model=model,\n",
    "        neuron_max_acts=max_text_data['neuron_max_acts'],\n",
    "        dataset=dataset,\n",
    "        device=device,\n",
    "        num_samples=30,\n",
    "        act_ratio=0.8,\n",
    "    )\n",
    "\n",
    "output = {\n",
    "    'parameters': parameters,\n",
    "    'neuron_to_trunc_data': neuron_20_examples,\n",
    "    'prior_filename': filename,\n",
    "}\n",
    "\n",
    "\n",
    "train_string = \"train\" if train else \"test_fr\"\n",
    "# Save json to ../experiment_data/2_max_activating_texts\n",
    "timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime(int(time.time())))\n",
    "new_filename = f\"{timestamp}_{model_name}_{train_string}.json\"\n",
    "\n",
    "with open(f'./experiment_data/3_trimmed_texts/{new_filename}', 'w') as f:\n",
    "    json.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'parameters': parameters,\n",
    "    'neuron_to_trunc_data': neuron_20_examples,\n",
    "    'prior_filename': filename,\n",
    "}\n",
    "\n",
    "\n",
    "train_string = \"train\" if train else \"test\"\n",
    "# Save json to ../experiment_data/2_max_activating_texts\n",
    "timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime(int(time.time())))\n",
    "new_filename = f\"{timestamp}_{model_name}_{train_string}.json\"\n",
    "\n",
    "with open(f'./experiment_data/3_trimmed_texts/{new_filename}', 'w') as f:\n",
    "    json.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
